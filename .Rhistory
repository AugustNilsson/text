textImportWordsPlot <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
output_vectors_sw <- list()
# Get word-embeddings for all individual-words (which is used for the word plot)
# Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = " ")
# Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
# Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
# Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify = T)
# Create dataframe with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = singlewords$words,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
# Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we <- tibble::as_tibble(singlewords_we1)
# Add the single words embeddings
output_vectors_sw$singlewords_we <- singlewords_we
output_vectors_sw
}
wordembeddings <- textImportText(sq_data_tutorial)
wordembeddings_sw
wordembeddings_text <- textImportText(sq_data_tutorial)
wordembeddings_sw <- textImportWordsPlot(sq_data_tutorial)
wordembeddings_text
wordembeddings_sw
wordembeddings_sw1 <- textImportWordsPlot(sq_data_tutorial)
colnames(textImportWordsPlot)
textImportWordsPlot
colnames(wordembeddings_sw1)
# This function it creating a decontextualised embedding for each single word
textImportWordsPlot <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
output_vectors_sw <- list()
# Get word-embeddings for all individual-words (which is used for the word plot)
# Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = " ")
# Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
# Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
# Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify = T)
# Create dataframe with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = singlewords$words,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
# Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we <- tibble::as_tibble(singlewords_we1)
# Add the single words embeddings
output_vectors_sw <- list()
output_vectors_sw$singlewords_we <- singlewords_we
output_vectors_sw
}
wordembeddings_sw1 <- textImportWordsPlot(sq_data_tutorial)
wordembeddings_sw1
wordembeddings_text$singlewords <- wordembeddings_sw1
wordembeddings_text
wordembeddings_text <- wordembeddings_sw1
wordembeddings_text
wordembeddings_text <- textImportText(sq_data_tutorial)
wordembeddings_sw <- textImportWordsPlot(sq_data_tutorial)
wordembeddings_text1 <- wordembeddings_text
wordembeddings_text1 <- wordembeddings_text
wordembeddings_sw1 <- wordembeddings_sw
wordembeddings_text1$singlewords_we2 <- wordembeddings_sw1
wordembeddings_text1
wordembeddings_sw1
wordembeddings_text1$singlewords_we2 <- wordembeddings_sw1$singlewords_we
wordembeddings_text1
textImport <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
...) {
wordembeddings_text <- textImportText(x)
wordembeddings_sw <- textImportWordsPlot(x)
wordembeddings_text$singlewords_we <- wordembeddings_sw$singlewords_we
wordembeddings_text
}
textImport <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
...) {
wordembeddings_text <- textImportText(x)
wordembeddings_sw <- textImportWordsPlot(x)
wordembeddings_text$singlewords_we <- wordembeddings_sw$singlewords_we
wordembeddings_text
}
wordembeddings <- textImport(sq_data_tutorial)
wordembeddings
wordembeddings <- textImport(sq_data_tutorial, "bert_base_multilingual_cased")
wordembeddings
gg <- "Jag mår bra tack"
wordembeddings <- textImport(gg, "bert_base_multilingual_cased")
gg
sq_data_tutorial
gg <- c("Jag mår bra tack", "Jag är lycklig")
wordembeddings <- textImport(gg, "bert_base_multilingual_cased")
gg <- tibble("Jag mår bra tack", "Jag är lycklig")
wordembeddings <- textImport(gg, "bert_base_multilingual_cased")
wordembeddings
wordembeddings <- textImport(gg, "bert_base")
wordembeddings
wordembeddings <- textImport(gg, "bert_base_uncased")
wordembeddings
gg <- tibble("är mår öde", "Jag är lycklig")
wordembeddings <- textImport(gg, "bert_base_uncased")
wordembeddings
wordembeddings <- textImportText(gg, "bert_base_multilingual_cased")
wordembeddings <- textImport(gg, "bert_base_multilingual_cased")
wordembeddings
BERT_feats
view(BERT_feats)
View(BERT_feats)
BERT_feats
as.data.frame(BERT_feats)
BERT_feats$output
BERT_feats$output$token
wordembeddings <- textImportText(gg, "bert_base_multilingual_cased")
# Swedish test
ggr <- tibble("hello", "you")
wordembeddings3 <- textImportText(ggr, "bert_base_multilingual_cased")
wordembeddings4 <- textImportText(ggr, "bert_base_uncased")
ggr1 <- tibble("hello", "you", "are")
ggr2 <- tibble("how", "you", "sick")
wordembeddings3 <- textImportText(ggr1, "bert_base_uncased")
wordembeddings4 <- textImportText(ggr2, "bert_base_uncased")
wordembeddings3
wordembeddings4
wordembeddings3$`"you"`
wordembeddings4$`"you"`
wordembeddings3$`"you"` %in% wordembeddings4$`"you"`
table(wordembeddings3$`"you"` %in% wordembeddings4$`"you"`)
# This function it creating a decontextualised embedding for each single word
textImportDecontext <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create list to store output
output_vectors_sw <- list()
# Get word-embeddings for all individual-words (which is used for the word plot)
# Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = " ")
# Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
# Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
# Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify = T)
# Create dataframe with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = singlewords$words,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
# Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we <- tibble::as_tibble(singlewords_we1)
# Add the single words embeddings
output_vectors_sw <- list()
output_vectors_sw$singlewords_we <- singlewords_we
output_vectors_sw
}
# Decontextualised test
ggr1 <- tibble("hello", "you", "are")
ggr2 <- tibble("how", "you", "sick")
wordembeddings3 <- textImportDecontext(ggr1, "bert_base_uncased")
wordembeddings4 <- textImportDecontext(ggr2, "bert_base_uncased")
table(wordembeddings3$`"you"` %in% wordembeddings4$`"you"`)
wordembeddings3
wordembeddings3[3]
wordembeddings3[,3]
wordembeddings3[3,]
wordembeddings3
wordembeddings3[3, 1]
wordembeddings3[, 1]
table(wordembeddings3 %in% wordembeddings4)
wordembeddings4
wordembeddings3[1]
table(wordembeddings3[1] %in% wordembeddings4[1])
table(wordembeddings3[3] %in% wordembeddings4[3])
help(make_examples_simple)
input_ex <- make_examples_simple(c(
"Here are some words.",
"Here are some more words."
))
input_ex2 <- make_examples_simple(list(
c(
"First sequence, first segment.",
"First sequence, second segment."
),
c(
"Second sequence, first segment.",
"Second sequence, second segment."
)
))
input_ex
# List to send to BERT with text a and text b; where the text is split up to 512/2 tokens and
#then a little bit less to the sentence sign including: .!?.
help(extract_features)
# OK
input_ex3 <- make_examples_simple(list(
c(
"First sequence, first segment.",
"First sequence, second segment."
),
c(
"Second sequence, first segment.",
"Second sequence, second segment."
),
c(
"Third sequence, first segment.",
"Third sequence, second segment."
)
))
input_ex3
# OK list
input_ex4 <- make_examples_simple(list(
c(
"First sentence, first time. [text_a]",
"Second sentence, first time.[text_b]"
),
c(
"Second sentence, second time. [text_a]",
"Third sentence, first time [text_b]"
),
c(
"Third sentence, second time. [text_a]",
"Fourth sentence, first segment. [text_b]"
)
))
input_ex4
BERT_feats4 <- extract_features(
examples = input_ex4,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
BERT_feats4
BERT_feats4$token
BERT_feats4
BERT_feats4$output$token
OK_text <- tibble("First sentence, first time. [text_a]
Second sentence, first time.[text_b]
Second sentence, second time. [text_a]
Third sentence, first time [text_b]
Third sentence, second time. [text_a]
Fourth sentence, first segment. [text_b])
BERT_feats4 <- extract_features(
examples = input_ex4,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
BERT_feats4
BERT_feats4$output$token
tokens <- tokenize_text(text = "Who doesn't like tacos?",
ckpt_dir = BERT_PRETRAINED_DIR)
OK_text <- tibble("First sentence, first time. [text_a]
Second sentence, first time.[text_b]
Second sentence, second time. [text_a]
Third sentence, first time [text_b]
Third sentence, second time. [text_a]
Fourth sentence, first segment. [text_b]")
OK_text
OK_text <- tibble("First sentence, first time. Second sentence, first time. Third sentence, first time. Fourth sentence, first segment.")
OK_text <- tibble("First sentence, first time. Second sentence, first time. Third sentence, first time. Fourth sentence, first segment.")
OK_text
OK_tokens <- tokenize_text(text = OK_text,
ckpt_dir = BERT_PRETRAINED_DIR)
OK_tokens
OK_tokens[1:4]
OK_tokens[4]
OK_tokens
unlist(OK_tokens)[4]
unlist(OK_tokens)
unlist(OK_tokens)
as.list(OK_tokens)
as.tibble(OK_tokens)
as_tibble(OK_tokens)
OK_tokens1 <- as_tibble(OK_tokens)
OK_tokens1[4]
OK_tokens1[,4]
OK_tokens1[4,]
OK_tokens1[1:4,]
OK_tokens1[1:6,]
OK_text <- tibble("First sentence. Second sentence. Third sentence. Fourth sentence.")
OK_text
OK_tokens <- tokenize_text(text = OK_text,
ckpt_dir = BERT_PRETRAINED_DIR)
OK_tokens
OK_tokens1 <- as_tibble(OK_tokens)
OK_tokens1[1:6,]
OK_tokens1[1:8,]
BERT_feats4
BERT_feats4_test <- BERT_feats4
min(BERT_feats4_test)
min(BERT_feats4_test$output)
min(BERT_feats4_test$output[1])
min_vector <- lapply(BERT_feats4_test$output, min)
min_vector
min_vector <- unlist(lapply(BERT_feats4_test$output, min))
min_vector
min_vector <- unlist(map(BERT_feats4_test$output, min))
min_vector
min_vector <- map(BERT_feats4_test$output, min)
min_vector
max_vector <- unlist(map(BERT_feats4_test$output, max))
max_vector
mean_vector <- unlist(map(BERT_feats4_test$output, mean))
mean_vector
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(BERT_feats4_test$output, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(BERT_feats4_test$output, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(BERT_feats4_test$output, mean))
} else {
x
}
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
}
textEmbeddingAggregation(BERT_feats4_test$output, aggregation == "max")
textEmbeddingAggregation(BERT_feats4_test$output, aggregation == "max")
textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "max")
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(BERT_feats4_test$output, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(BERT_feats4_test$output, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(BERT_feats4_test$output, mean))
} else {
x
}
}
textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "max")
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "max")
testingfunction
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "min")
testingfunction
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "mean")
testingfunction
BERT_feats4_test$output
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
# Aggregation of word embeddings; WHAT HAPPENS IF ONE TAKES SEVERAL LAYERS
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "CLS")
testingfunction
BERT_feats4_test$output
# This version is importing the entire cell/response/paragraph in one; but only the 512-first tokens.
# However, should make one that take in individual words without context; and another taking in sentences that are summed up?
textImportText <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
aggregation = "mean",
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
BERT_feats <- list()
output_vectors <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Extract BERT feature; help(extract_features) help(make_examples_simple)
BERT_feats[[i]] <- RBERT::extract_features(
examples = x_characters[[i]],
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
#output_vectors[[i]] <- BERT_feats[[i]]$output %>%
#  dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
output_vectors[[i]] <- textEmbeddingAggregation(BERT_feats[[i]]$output, aggregation = aggregation)
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
output_vectors
}
# Swedish test GIVE ERROR
gg <- tibble("I am fine", "How are you")
wordembeddings2 <- textImportText(gg, "bert_base_uncased")
wordembeddings2
sq_data_tutorial
wordembeddings <- textImportText(sq_data_tutorial)
