x_characters5 <- gsub("[\r\]", " ", x_characters4)
#Tokenize into single words
x_characters5 <- tokenizers::tokenize_words(x_characters4)
x_characters5
#Create datafrema with single words and frequency
x_characters6 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters5), " ")))))
x_characters6
#Remove \n
x_characters4 <- gsub("[\r\n]", " ", x_characters3)
x_characters4
#Remove \n
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
x_characters4
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
x_characters3
#Remove \n
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
x_characters4
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
#Tokenize into single words
x_characters6 <- tokenizers::tokenize_words(x_characters5)
#Create datafrema with single words and frequency
x_characters7 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters6), " ")))))
x_characters7
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
#Tokenize into single words
x_characters6 <- tokenizers::tokenize_words(x_characters5)
x_characters7 <- tokenizers::tokenize_words(x_characters6)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
x_characters8
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
#Tokenize into single words
x_characters6 <- tokenizers::tokenize_words(x_characters5)
x_characters7 <- tokenizers::tokenize_words(x_characters6)
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\r\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
x_characters8
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
x_characters8
x_characters9 <- as.tibble(x_characters8)
x_characters9 <- as_tibble(x_characters8)
x_characters9
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
x_characters8
x_characters9 <- tibble(x_characters8$Var1, x_characters8$Freq)
x_characters9
x_characters9 <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(x_characters9) <- c("words", "n")
x_characters9$words <- as.character(x_characters9$words)
x_characters9
x_characters8
x_characters6
#Tokenize into single words
help(tokenize_words)
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
x_characters7
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
x_characters8
x_characters9 <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(x_characters9) <- c("words", "n")
x_characters9$words <- as.character(x_characters9$words)
x_characters9
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
x_characters$singlewords <- singlewords
singlewords
singlewords
x_characters_1 <- list(x_characters, singlewords)
x_characters_1
# Create lists
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
x_characters <- list(x_characters, singlewords)
# Create lists
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
x_characters
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
x_characters <- apply(x_characters, 2, as.list)
x_characters
x_characters <- dplyr::select_if(x, is.character)
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
x_characters11 <- apply(x_characters, 1, as.list)
x_characters11
x_characters
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
layer_indexes_REBERT = 12
batch_size_IBT = 2L
token_index_IBT = 1
layer_index_IBT = 12
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Download pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased")
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
x_characters
# Create lists
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
output_vectors
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1_sw <- singlewords$words
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1_sw <- singlewords$words
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1_sw),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
#Add frequency for each word
output_vectors_sw$n <- singlewords$n
output_vectors_sw
output_vectors_sw$words <- output_vectors_sw$words
output_vectors_sw$words
singlewords$words
output_vectors_sw$words <- singlewords$words
output_vectors_sw$words
output_vectors_sw
output_vectors
output_vectors$singlewords_we <- output_vectors_sw
output_vectors
#Add frequency for each word
singlewords_we <- cbind(singlewords, output_vectors_sw)
singlewords_we
#Add frequency for each word
singlewords_we <- tibble(cbind(singlewords, output_vectors_sw))
singlewords_we
singlewords_we <- as_tibble(singlewords_we)
singlewords_we
#Add frequency for each word
singlewords_we <- cbind(singlewords, output_vectors_sw)
singlewords_we
#Add frequency for each word
singlewords_we <- tibble(singlewords, output_vectors_sw)
singlewords_we
#Add frequency for each word
singlewords_we <- as_tibble(singlewords, output_vectors_sw)
singlewords_we
singlewords
output_vectors_sw
#Add frequency for each word
singlewords_we <- tibble(singlewords, output_vectors_sw)
# Add the single words embeddings
output_vectors$singlewords_we <- singlewords_we
output_vectors
# This version is importing the entire paragraph in one; but only the 512-first tokens.
textImport <- function(x, layer_indexes_REBERT = 12, batch_size_IBT = 2L, token_index_IBT = 1, layer_index_IBT = 12,  ...){
# Download pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased")
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
# Create lists
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
#Single words' word embeddings for semantic plots
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1_sw <- singlewords$words
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1_sw),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats_sw
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
#Add frequency for each word
singlewords_we <- tibble(singlewords, output_vectors_sw)
# Add the single words embeddings
output_vectors$singlewords_we <- singlewords_we
output_vectors
}
devtools::document()
devtools::document()
devtools::document()
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
x_characters2
devtools::document()
usethis::use_package("stringr")
usethis::use_package("tidyr")
devtools::document()
library(purr)
install.packages("purr")
sq_data_outcome <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/Orignal for DLATK/outcome_US_3_AllSQanswers.csv", sep=",")
sq_data_text <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/uploaded/2 Study1-3_for_DLATK_TEXTdata.csv", sep=",")
#install.packages("lazy-load")
library(tidyverse)
library(data.table)
sq_data_text_long <- dcast(setDT(sq_data_text), X_DLATKid ~ X_wh1ws2th3ts4, value.var=c("X_text", "Study"))
head(sq_data_text_long)
nrow(sq_data_text_long)
sq_data <- merge(sq_data_text_long, sq_data_outcome, by.x="X_DLATKid", by.y="user_id")
head(sq_data)
nrow(sq_data)
sq_data <- as_tibble(sq_data)
sq_data <- rename(sq_data, harmonywords = X_text_1)
sq_data <- rename(sq_data, satisfactionwords = X_text_2)
sq_data <- rename(sq_data, harmonytexts = X_text_3)
sq_data <- rename(sq_data, satisfactiontexts = X_text_4)
sq_data
sq_data <- sq_data %>%
mutate_at(vars(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts), as.character)
#Data for tutorial
sq_data_tutorial <- sq_data %>%
select(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts, hilstotal, swlstotal, age, gender) %>%
slice(1:10)
x=sq_data_tutorial
layer_indexes_REBERT = 12
batch_size_IBT = 2L
token_index_IBT = 1
layer_index_IBT = 12
# Download pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased")
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
x_characters
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
x_characters2
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords
singlewords$words <- as.character(singlewords$words)
singlewords
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1_sw <- singlewords$words
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1_sw),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats_sw
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
singlewords
output_vectors_sw
singlewords
output_vectors_sw
singlewords_we
#Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we1
singlewords_we <- as_tibble(singlewords_we1)
singlewords_we
library(text)
