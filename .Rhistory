BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased")
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
x_characters
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
x_characters2
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords
singlewords$words <- as.character(singlewords$words)
singlewords
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1_sw <- singlewords$words
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1_sw),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats_sw
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
singlewords
output_vectors_sw
singlewords
output_vectors_sw
singlewords_we
#Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we1
singlewords_we <- as_tibble(singlewords_we1)
singlewords_we
library(text)
library(text)
library(text)
sq_data_outcome <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/Orignal for DLATK/outcome_US_3_AllSQanswers.csv", sep=",")
sq_data_text <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/uploaded/2 Study1-3_for_DLATK_TEXTdata.csv", sep=",")
#install.packages("lazy-load")
library(tidyverse)
library(data.table)
sq_data_text_long <- dcast(setDT(sq_data_text), X_DLATKid ~ X_wh1ws2th3ts4, value.var=c("X_text", "Study"))
head(sq_data_text_long)
nrow(sq_data_text_long)
sq_data <- merge(sq_data_text_long, sq_data_outcome, by.x="X_DLATKid", by.y="user_id")
head(sq_data)
nrow(sq_data)
sq_data <- as_tibble(sq_data)
sq_data <- rename(sq_data, harmonywords = X_text_1)
sq_data <- rename(sq_data, satisfactionwords = X_text_2)
sq_data <- rename(sq_data, harmonytexts = X_text_3)
sq_data <- rename(sq_data, satisfactiontexts = X_text_4)
sq_data
sq_data <- sq_data %>%
mutate_at(vars(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts), as.character)
#Data for tutorial
sq_data_tutorial <- sq_data %>%
select(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts, hilstotal, swlstotal, age, gender) %>%
slice(1:10)
x <- sq_data_tutorial[1:2, 1:2]
x
layer_indexes_REBERT = 12
batch_size_IBT = 2L
token_index_IBT = 1
layer_index_IBT = 12
# Download pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased")
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
x_characters
BERT_feats <- list()
output_vectors <- list()
tokenized_sentences1 <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT, ...)
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Tokenize sentences to list
tokenized_sentences1[[i]] <- x_characters[[i]]
# Extract BERT feature
BERT_feats[[i]] <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1[[i]]),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
output_vectors
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
# Get word-embeddings for all individual-words
#Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = ' ')
#Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
#Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
#Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify=T)
#Create datafrema with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Tokenize sentences to list
#    tokenized_sentences1[[i]] <- mapply(tokenize_sentences, x_characters[[i]])
tokenized_sentences1_sw <- singlewords$words
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = RBERT::make_examples_simple(tokenized_sentences1_sw),
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_REBERT,
batch_size = batch_size_IBT) #, ...
BERT_feats_sw
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_IBT, layer_index == layer_index_IBT)
#Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we1
singlewords_we <- as_tibble(singlewords_we1)
singlewords_we
# Add the single words embeddings
output_vectors$singlewords_we <- singlewords_we
output_vectors
library(text)
x
x_test <- textImport(x)
x_test
library(text)
library(text)
library(text)
library(text)
library(text)
#You can download R and R-studio for free
#This cript includes code for combining word responses for semantic questions, adding rating scales items together,
#computing Cronbach's alpha and McDonald's omega, correlations, and saving the data to a file that can be importarted to Semantic Excel
#If something does not work, try to first Google for a solution as there are great resources online and it is very important to learn how to find this information when working in R
#This sets your working directory; that is, the folder that you work in.
#So on your computer, create a folder and add your raw data files in it. Then copy and past the path to the folder below.
setwd("C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data")
#First time you open a libray you may eed to install it on your computer. So remove the # sign below and run the installation code line; then run the library line after installation. If you close down R you only need to run the library code again
#install.packages("Hmisc")
library(tidyverse)
#Open rest of libraries (install them as needed)
library(car)
library(psych)
# Load your data-file
data_C1 <- read.csv(file="C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/Activities I/Activities I raw.csv", sep = ";", dec = ".", header=TRUE)
nrow(data_C1)
head(data_C1)
nrow(data_C1)
data_C2 <- read.csv(file="C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/Activities II/Activities II raw.csv", sep = ";", dec = ".", header=TRUE)
nrow(data_C2)
head(data_C2)
nrow(data_C2)
#merge the two files
data <- rbind(data_C1, data_C2)
nrow(data)
#Remove not-submitted responses; that is, remove those rows that do not have anythinig in the submitdate column
ds <- data[!(data$submitdate==" "), ]
nrow(ds)
#Change name of the ID column to participant_id; That is, is it called ID in the dataset from limesurvey? Otherwise change "ID" below to the name it has in the ds dataframe.
colnames(ds)[which(names(ds) == "WorkerID")] <- "participant_id"
ds$participant_id
#Read Prolificdata
#5ceb = ASDFASDFASDFAFs
prolific_1 <- read.csv("C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/prolific_export_5de7d83a84d974000c56f52d_Activities_I.csv")
prolific_2 <- read.csv("C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/prolific_export_5de7ec7050f521766471eda8_Activities_II.csv")
prolific <- rbind(prolific_1, prolific_2)
head(prolific)
nrow(prolific)
prolific$participant_id
#Merging the two files together
ds <- merge(ds, prolific, "participant_id")
nrow(ds)
data_C1 <- read.csv(file="/Users/oscar/Desktop/Undervisning/2019 HT/handledning/August och Erik/Activities and well-being data 5/Activities I/Activities I raw.csv", sep = ";", dec = ".", header=TRUE)
#data_C1 <- read.csv(file="C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/Activities I/Activities I raw.csv", sep = ";", dec = ".", header=TRUE)
nrow(data_C1)
head(data_C1)
nrow(data_C1)
data_C2 <- read.csv(file="/Users/oscar/Desktop/Undervisning/2019 HT/handledning/August och Erik/Activities and well-being data 5/Activities II/Activities II raw.csv", sep = ";", dec = ".", header=TRUE)
#data_C2 <- read.csv(file="C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/Activities II/Activities II raw.csv", sep = ";", dec = ".", header=TRUE)
nrow(data_C2)
head(data_C2)
nrow(data_C2)
#merge the two files
data <- rbind(data_C1, data_C2)
nrow(data)
#Remove not-submitted responses; that is, remove those rows that do not have anythinig in the submitdate column
ds <- data[!(data$submitdate==" "), ]
nrow(ds)
#Change name of the ID column to participant_id; That is, is it called ID in the dataset from limesurvey? Otherwise change "ID" below to the name it has in the ds dataframe.
colnames(ds)[which(names(ds) == "WorkerID")] <- "participant_id"
ds$participant_id
#Read Prolificdata
#5ceb = ASDFASDFASDFAFs
prolific_1 <- read.csv("/Users/oscar/Desktop/Undervisning/2019 HT/handledning/August och Erik/Activities and well-being data 5/prolific_export_5de7d83a84d974000c56f52d_Activities_I.csv")
prolific_2 <- read.csv("/Users/oscar/Desktop/Undervisning/2019 HT/handledning/August och Erik/Activities and well-being data 5/prolific_export_5de7ec7050f521766471eda8_Activities_II.csv")
#prolific_1 <- read.csv("C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/prolific_export_5de7d83a84d974000c56f52d_Activities_I.csv")
#prolific_2 <- read.csv("C:/Users/erikh/Google Drive/LU/AugEri Kandidatuppsats h?st/Data/Activities and well-being data/prolific_export_5de7ec7050f521766471eda8_Activities_II.csv")
prolific <- rbind(prolific_1, prolific_2)
head(prolific)
nrow(prolific)
prolific$participant_id
#Merging the two files together
ds <- merge(ds, prolific, "participant_id")
nrow(ds)
#For open data (removing prolific ID)
colnames(ds)
ds_opendata <- subset(df, select=-c(participant_id, id, token, lastpage, startlanguage,
session_id, prolific_score, entered_code))
library(tidyverse)
ds_opendata <- subset(df, select=-c(participant_id, id, token, lastpage, startlanguage,
session_id, prolific_score, entered_code))
library(dplyr)
ds_opendata <- subset(df, select=-c(participant_id, id, token, lastpage, startlanguage,
session_id, prolific_score, entered_code))
subset(df, select=-c(participant_id, id, token, lastpage, startlanguage,
session_id, prolific_score, entered_code))
subset(df, select=-c(participant_id, id, token, lastpage, startlanguage, session_id, prolific_score, entered_code))
ds_opendata <- subset.data.frame::subset(df, select=-c(participant_id, id, token, lastpage, startlanguage, session_id, prolific_score, entered_code))
ds_opendata <- base::subset(df, select=-c(participant_id, id, token, lastpage, startlanguage, session_id, prolific_score, entered_code))
#ds_opendata <- base::subset(df, select=-c(participant_id, id, token, lastpage, startlanguage, session_id, prolific_score, entered_code))
ds_opendata <- ds[ , -which(names(ds) %in% c("participant_id", "id", "token", "lastpage", "startlanguage", "session_id", "prolific_score", "entered_code"))]
ds_opendata
write_csv(ds_opendata, "ds_opendata.csv")
write_csv(ds_opendata, "/Users/oscar/Desktop/Undervisning/2019 HT/handledning/August och Erik/Activities and well-being data 5/ds_opendata.csv")
####Control questions
#First if any one is NA we give them the CORRECT answer as they did not answer the questions (i.e., if NA they have not answered it if it was a forced-answer question)
head(ds)
ds$Control[is.na(ds$Control)] <- 4
#Then remove those not answer correctly
ds <- ds[ds[,"Control"]==4,]
nrow(ds)
#Next control item
ds$Control2[is.na(ds$Control2)] <- 4
ds <- ds[ds[,"Control2"]==4,]
nrow(ds)
#Demographics
head(ds)
mean(ds$age)
sd(ds$age)
range(ds$age)
table(ds$Gender)
table(ds$Country.of.Birth)
order(table(ds$Country.of.Birth))
table(ds$Sex)
table(ds$Student.Status)
table(ds$Employment.Status)
mean(as.numeric(ds$Socioeconomic.Status))
sd(as.numeric(ds$Socioeconomic.Status))
library(psych)
psych::describe(as.numeric(ds$Socioeconomic.Status))
#Survey completion time (see also complettion time below)
ds$startdate_1 <- as.POSIXct(ds$startdate, format='%m/%d/%Y %H:%M:%S')
ds$submitdate_1 <- as.POSIXct(ds$submitdate, format='%m/%d/%Y %H:%M:%S')
#Create Time difference and remove time stamps
ds$time_Dif <- difftime(ds$submitdate_1, ds$startdate_1, units="mins")
mean(ds$time_Dif)
sd(ds$time_Dif)
head(ds)
#Wake Up Time
#Oscars f?rsta id?
ds$WakeUpClean
WakeUpClean <- as.POSIXct(ds$WakeUpClean, format = '"%H%M"')
WakeUpClean
mean(ds$WakeUpClean)
#Oscars andra id?
ds$WakeUpClean
ds$WakeUpClean2 <- as.character(ds$WakeUpClean)
ds$WakeUpClean2
WakeUpClean <- as.POSIXct(ds$WakeUpClean2, format = '%H%M')
ds$WakeUpClean
mean(ds$WakeUpClean)
#BedTime
ds$GoToSleepClean
ds$WakeUpClean
WakeUpClean <- as.POSIXct(ds$WakeUpClean, format = '"%H%M"')
WakeUpClean
mean(WakeUpClean)
GoToSleepClean <- as.POSIXct(ds$GoToSleepClean, format = '"%H%M"')
GoToSleepClean
mean(GoToSleepClean)
mean(GoToSleepClean, rm.na=TRUE)
mean(GoToSleepClean, na.rm=TRUE)
sd(GoToSleepClean)
mean(WakeUpClean)
sd(WakeUpClean)
mean(GoToSleepClean, na.rm=TRUE)
sd(GoToSleepClean, na.rm=TRUE)
#You can download R and R-studio for free
citeation(psych)
#You can download R and R-studio for free
citation(psych)
#You can download R and R-studio for free
citation("psych")
citation("Hmisc")
#You can download R and R-studio for free
citation("psych")
citation("car")
citation("tidyverse")
citation("dplyr")
citation("Hmisc")
citation()
library(text)
usethis::use_package("BSDA")
usethis::use_package("ggplot2")
usethis::use_package("ggrepel")
devtools::document()
library(text)
sq_data_outcome <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/Orignal for DLATK/outcome_US_3_AllSQanswers.csv", sep=",")
sq_data_text <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/uploaded/2 Study1-3_for_DLATK_TEXTdata.csv", sep=",")
#install.packages("lazy-load")
library(tidyverse)
library(data.table)
sq_data_text_long <- dcast(setDT(sq_data_text), X_DLATKid ~ X_wh1ws2th3ts4, value.var=c("X_text", "Study"))
head(sq_data_text_long)
nrow(sq_data_text_long)
sq_data <- merge(sq_data_text_long, sq_data_outcome, by.x="X_DLATKid", by.y="user_id")
head(sq_data)
nrow(sq_data)
sq_data <- as_tibble(sq_data)
sq_data <- rename(sq_data, harmonywords = X_text_1)
sq_data <- rename(sq_data, satisfactionwords = X_text_2)
sq_data <- rename(sq_data, harmonytexts = X_text_3)
sq_data <- rename(sq_data, satisfactiontexts = X_text_4)
sq_data
sq_data <- sq_data %>%
mutate_at(vars(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts), as.character)
#Data for tutorial
sq_data_tutorial <- sq_data %>%
select(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts, hilstotal, swlstotal, age, gender) %>%
slice(1:100)
wordembeddings <- textImport(sq_data_tutorial)
saveRDS(wordembeddings, "/Users/oscar/Desktop/0 Studies/5 R statistical semantics package/text/data/wordembeddings4_100.rda")
saveRDS(wordembeddings, "/Users/oscar/Desktop/0 Studies/5 R statistical semantics package/text/data/wordembeddings4_100.rda")
wordembeddings4_100 <- wordembeddings
wordembeddings4_100
usethis::use_data(wordembeddings4_100)
usethis::use_data(wordembeddings4_100, overwrite = TRUE)
library(text)
devtools::document()
usethis::use_package("scales")
wordembeddings4_100[, 1:2]
wordembeddings4_100
sq_data_tutorial4_100[, 1:2]
sq_data_tutorial4_100[1:2,]
wordembeddings4_100[1:10,]
wordembeddings4_10
sq_data_tutorial8_10
sq_data_outcome <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/Orignal for DLATK/outcome_US_3_AllSQanswers.csv", sep=",")
sq_data_text <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/uploaded/2 Study1-3_for_DLATK_TEXTdata.csv", sep=",")
#install.packages("lazy-load")
library(tidyverse)
library(data.table)
sq_data_text_long <- dcast(setDT(sq_data_text), X_DLATKid ~ X_wh1ws2th3ts4, value.var=c("X_text", "Study"))
head(sq_data_text_long)
nrow(sq_data_text_long)
sq_data <- merge(sq_data_text_long, sq_data_outcome, by.x="X_DLATKid", by.y="user_id")
head(sq_data)
nrow(sq_data)
sq_data <- as_tibble(sq_data)
sq_data <- rename(sq_data, harmonywords = X_text_1)
sq_data <- rename(sq_data, satisfactionwords = X_text_2)
sq_data <- rename(sq_data, harmonytexts = X_text_3)
sq_data <- rename(sq_data, satisfactiontexts = X_text_4)
sq_data
sq_data <- sq_data %>%
mutate_at(vars(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts), as.character)
#Data for tutorial
sq_data_tutorial <- sq_data %>%
select(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts, hilstotal, swlstotal, age, gender) %>%
slice(1:10)
wordembeddings <- textImport(sq_data_tutorial)
#saveRDS(wordembeddings, "/Users/oscar/Desktop/0 Studies/5 R statistical semantics package/text/data/wordembeddings4_10.rda")
wordembeddings4_10 <- wordembeddings
usethis::use_data(wordembeddings4_10)
devtools::document()
devtools::document()
devtools::document()
library(text)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
library(text)
library(text)
wordembeddings4_10
sq_data_tutorial8_10
numeric_data
#Data for tutorial
sq_data_tutorial <- sq_data %>%
select(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts, hilstotal, swlstotal, age, gender) %>%
slice(1:10)
sq_data_tutorial
sq_data_outcome <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/Orignal for DLATK/outcome_US_3_AllSQanswers.csv", sep=",")
sq_data_text <- read.csv("/Users/oscar/Dropbox/Semantic Measures and BERT/Harmony and Satisfaction data/uploaded/2 Study1-3_for_DLATK_TEXTdata.csv", sep=",")
#install.packages("lazy-load")
library(tidyverse)
library(data.table)
sq_data_text_long <- dcast(setDT(sq_data_text), X_DLATKid ~ X_wh1ws2th3ts4, value.var=c("X_text", "Study"))
head(sq_data_text_long)
nrow(sq_data_text_long)
sq_data <- merge(sq_data_text_long, sq_data_outcome, by.x="X_DLATKid", by.y="user_id")
head(sq_data)
nrow(sq_data)
sq_data <- as_tibble(sq_data)
sq_data <- rename(sq_data, harmonywords = X_text_1)
sq_data <- rename(sq_data, satisfactionwords = X_text_2)
sq_data <- rename(sq_data, harmonytexts = X_text_3)
sq_data <- rename(sq_data, satisfactiontexts = X_text_4)
sq_data
sq_data <- sq_data %>%
mutate_at(vars(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts), as.character)
#Data for tutorial
sq_data_tutorial <- sq_data %>%
select(harmonywords, satisfactionwords, harmonytexts, satisfactiontexts, hilstotal, swlstotal, age, gender) %>%
slice(1:10)
sq_data_tutorial
usethis::use_citation(sq_data_tutorial8_10)
usethis::use_data(sq_data_tutorial8_10)
usethis::use_data(sq_data_tutorial8_10, overwrite = TRUE)
wordembeddings <- textImport(sq_data_tutorial)
sq_data_tutorial8_10
wordembeddings <- textImport(sq_data_tutorial)
library(text)
wordembeddings <- textImport(sq_data_tutorial)
library(text)
wordembeddings <- textImport(sq_data_tutorial)
library(text)
wordembeddings <- textImport(sq_data_tutorial)
