x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
# Extract BERT feature
BERT_feats_sw <- RBERT::extract_features(
examples = singlewords$words,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences... These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors_sw <- BERT_feats_sw$output %>%
dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
# Add frequency for each word
singlewords_we1 <- cbind(singlewords, output_vectors_sw)
singlewords_we <- tibble::as_tibble(singlewords_we1)
# Add the single words embeddings
output_vectors_sw <- list()
output_vectors_sw$singlewords_we <- singlewords_we
output_vectors_sw
}
# Decontextualised test
ggr1 <- tibble("hello", "you", "are")
ggr2 <- tibble("how", "you", "sick")
wordembeddings3 <- textImportDecontext(ggr1, "bert_base_uncased")
wordembeddings4 <- textImportDecontext(ggr2, "bert_base_uncased")
table(wordembeddings3$`"you"` %in% wordembeddings4$`"you"`)
wordembeddings3
wordembeddings3[3]
wordembeddings3[,3]
wordembeddings3[3,]
wordembeddings3
wordembeddings3[3, 1]
wordembeddings3[, 1]
table(wordembeddings3 %in% wordembeddings4)
wordembeddings4
wordembeddings3[1]
table(wordembeddings3[1] %in% wordembeddings4[1])
table(wordembeddings3[3] %in% wordembeddings4[3])
help(make_examples_simple)
input_ex <- make_examples_simple(c(
"Here are some words.",
"Here are some more words."
))
input_ex2 <- make_examples_simple(list(
c(
"First sequence, first segment.",
"First sequence, second segment."
),
c(
"Second sequence, first segment.",
"Second sequence, second segment."
)
))
input_ex
# List to send to BERT with text a and text b; where the text is split up to 512/2 tokens and
#then a little bit less to the sentence sign including: .!?.
help(extract_features)
# OK
input_ex3 <- make_examples_simple(list(
c(
"First sequence, first segment.",
"First sequence, second segment."
),
c(
"Second sequence, first segment.",
"Second sequence, second segment."
),
c(
"Third sequence, first segment.",
"Third sequence, second segment."
)
))
input_ex3
# OK list
input_ex4 <- make_examples_simple(list(
c(
"First sentence, first time. [text_a]",
"Second sentence, first time.[text_b]"
),
c(
"Second sentence, second time. [text_a]",
"Third sentence, first time [text_b]"
),
c(
"Third sentence, second time. [text_a]",
"Fourth sentence, first segment. [text_b]"
)
))
input_ex4
BERT_feats4 <- extract_features(
examples = input_ex4,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
BERT_feats4
BERT_feats4$token
BERT_feats4
BERT_feats4$output$token
OK_text <- tibble("First sentence, first time. [text_a]
Second sentence, first time.[text_b]
Second sentence, second time. [text_a]
Third sentence, first time [text_b]
Third sentence, second time. [text_a]
Fourth sentence, first segment. [text_b])
BERT_feats4 <- extract_features(
examples = input_ex4,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
BERT_feats4
BERT_feats4$output$token
tokens <- tokenize_text(text = "Who doesn't like tacos?",
ckpt_dir = BERT_PRETRAINED_DIR)
OK_text <- tibble("First sentence, first time. [text_a]
Second sentence, first time.[text_b]
Second sentence, second time. [text_a]
Third sentence, first time [text_b]
Third sentence, second time. [text_a]
Fourth sentence, first segment. [text_b]")
OK_text
OK_text <- tibble("First sentence, first time. Second sentence, first time. Third sentence, first time. Fourth sentence, first segment.")
OK_text <- tibble("First sentence, first time. Second sentence, first time. Third sentence, first time. Fourth sentence, first segment.")
OK_text
OK_tokens <- tokenize_text(text = OK_text,
ckpt_dir = BERT_PRETRAINED_DIR)
OK_tokens
OK_tokens[1:4]
OK_tokens[4]
OK_tokens
unlist(OK_tokens)[4]
unlist(OK_tokens)
unlist(OK_tokens)
as.list(OK_tokens)
as.tibble(OK_tokens)
as_tibble(OK_tokens)
OK_tokens1 <- as_tibble(OK_tokens)
OK_tokens1[4]
OK_tokens1[,4]
OK_tokens1[4,]
OK_tokens1[1:4,]
OK_tokens1[1:6,]
OK_text <- tibble("First sentence. Second sentence. Third sentence. Fourth sentence.")
OK_text
OK_tokens <- tokenize_text(text = OK_text,
ckpt_dir = BERT_PRETRAINED_DIR)
OK_tokens
OK_tokens1 <- as_tibble(OK_tokens)
OK_tokens1[1:6,]
OK_tokens1[1:8,]
BERT_feats4
BERT_feats4_test <- BERT_feats4
min(BERT_feats4_test)
min(BERT_feats4_test$output)
min(BERT_feats4_test$output[1])
min_vector <- lapply(BERT_feats4_test$output, min)
min_vector
min_vector <- unlist(lapply(BERT_feats4_test$output, min))
min_vector
min_vector <- unlist(map(BERT_feats4_test$output, min))
min_vector
min_vector <- map(BERT_feats4_test$output, min)
min_vector
max_vector <- unlist(map(BERT_feats4_test$output, max))
max_vector
mean_vector <- unlist(map(BERT_feats4_test$output, mean))
mean_vector
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(BERT_feats4_test$output, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(BERT_feats4_test$output, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(BERT_feats4_test$output, mean))
} else {
x
}
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
output_vectors[[i]] <- BERT_feats[[i]]$output %>%
dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
}
textEmbeddingAggregation(BERT_feats4_test$output, aggregation == "max")
textEmbeddingAggregation(BERT_feats4_test$output, aggregation == "max")
textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "max")
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(BERT_feats4_test$output, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(BERT_feats4_test$output, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(BERT_feats4_test$output, mean))
} else {
x
}
}
textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "max")
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "max")
testingfunction
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "min")
testingfunction
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "mean")
testingfunction
BERT_feats4_test$output
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
# Aggregation of word embeddings; WHAT HAPPENS IF ONE TAKES SEVERAL LAYERS
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
testingfunction <- textEmbeddingAggregation(BERT_feats4_test$output, aggregation = "CLS")
testingfunction
BERT_feats4_test$output
# This version is importing the entire cell/response/paragraph in one; but only the 512-first tokens.
# However, should make one that take in individual words without context; and another taking in sentences that are summed up?
textImportText <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
aggregation = "mean",
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
BERT_feats <- list()
output_vectors <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Extract BERT feature; help(extract_features) help(make_examples_simple)
BERT_feats[[i]] <- RBERT::extract_features(
examples = x_characters[[i]],
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
#output_vectors[[i]] <- BERT_feats[[i]]$output %>%
#  dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
output_vectors[[i]] <- textEmbeddingAggregation(BERT_feats[[i]]$output, aggregation = aggregation)
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
output_vectors
}
# Swedish test GIVE ERROR
gg <- tibble("I am fine", "How are you")
wordembeddings2 <- textImportText(gg, "bert_base_uncased")
wordembeddings2
sq_data_tutorial
wordembeddings <- textImportText(sq_data_tutorial)
#install.packages(reticulate)
#.rs.restartR()
library(reticulate)
#By default, reticulate uses the version of Python found on your PATH (i.e. Sys.which("python")).
Sys.which("python")
use_condaenv("anaconda3")
# Set the path to the Python executable file
use_python("/opt/anaconda3/bin/python3", required = T)
# Check the version of Python.
py_config()
#setwd("/Users/oscar/Desktop/0_Studies/5 R statistical semantics package/")
source_python("/Users/oscar/Desktop/0 Studies/5 R statistical semantics package/text/R/huggingface_interface.py")
# Create parameter to pass to Python function
x  <-  "Where are you sweet embeddings, I'm waiting for you?"
x  <-  "Ön hör vår sång"
# Call created function
y  <-  hgTransformerGetEmbedding(text_strings = x,
pretrained_weights = 'bert-base-uncased',
tokenizer_class = BertTokenizer,
model_class = BertModel)
y
y[[1]][[1]]
y[[1]][[1]]
typeof(y)
str(y)
df <- list("I want", "The summer.")
df
# Call created function
y  <-  hgTransformerGetEmbedding(text_strings = df,
pretrained_weights = 'bert-base-uncased',
tokenizer_class = BertTokenizer,
model_class = BertModel)
y
wordembeddings2
# Swedish test GIVE ERROR
gg <- tibble("I am fine", "How are you")
library(tidyverse)
# Swedish test GIVE ERROR library(tidyverse)
gg <- tibble("I am fine", "How are you")
wordembeddings2 <- textImportText(gg, "bert_base_uncased")
library(text)
wordembeddings2 <- textImportText(gg, "bert_base_uncased")
wordembeddings2 <- textImport(gg, "bert_base_uncased")
wordembeddings2 <- textImport(gg)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
select_character_v_utf8 <- function(x){
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
# This makes sure that all variables are UTF-8 coded, since BERT wants it that way
x_characters <- tibble::as_tibble(purrr::map(x_characters, stri_encode, "", "UTF-8"))
}
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
# This version is importing the entire cell/response/paragraph in one; but only the 512-first tokens.
# However, should make one that take in individual words without context; and another taking in sentences that are summed up?
textImportText <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
aggregation = "mean",
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
BERT_feats <- list()
output_vectors <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Extract BERT feature; help(extract_features) help(make_examples_simple)
BERT_feats[[i]] <- RBERT::extract_features(
examples = x_characters[[i]],
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
#output_vectors[[i]] <- BERT_feats[[i]]$output %>%
#  dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
output_vectors[[i]] <- textEmbeddingAggregation(BERT_feats[[i]]$output, aggregation = aggregation)
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
output_vectors
}
# Swedish test GIVE ERROR library(tidyverse) library(text)
gg <- tibble("I am fine", "How are you")
wordembeddings2 <- textImportText(gg)
library(tensorflow)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
select_character_v_utf8 <- function(x){
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
# This makes sure that all variables are UTF-8 coded, since BERT wants it that way
x_characters <- tibble::as_tibble(purrr::map(x_characters, stri_encode, "", "UTF-8"))
}
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
library(tensorflow)
# This version is importing the entire cell/response/paragraph in one; but only the 512-first tokens.
# However, should make one that take in individual words without context; and another taking in sentences that are summed up?
textImportText <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
aggregation = "mean",
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
BERT_feats <- list()
output_vectors <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Extract BERT feature; help(extract_features) help(make_examples_simple)
BERT_feats[[i]] <- RBERT::extract_features(
examples = x_characters[[i]],
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
#output_vectors[[i]] <- BERT_feats[[i]]$output %>%
#  dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
output_vectors[[i]] <- textEmbeddingAggregation(BERT_feats[[i]]$output, aggregation = aggregation)
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
output_vectors
}
gg <- tibble("I am fine")
wordembeddings2 <- textImportText(gg)
library(stri_encode)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
select_character_v_utf8 <- function(x){
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
# This makes sure that all variables are UTF-8 coded, since BERT wants it that way
x_characters <- tibble::as_tibble(purrr::map(x_characters, stringi::stri_encode, "", "UTF-8"))
}
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max))
} else if (aggregation == "mean") {
mean_vector <- unlist(map(x, mean))
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
}
}
# This version is importing the entire cell/response/paragraph in one; but only the 512-first tokens.
# However, should make one that take in individual words without context; and another taking in sentences that are summed up?
textImportText <- function(x,
model = "bert_base_uncased",
layer_indexes_RBERT = 12,
batch_size = 2L,
token_index_filter = 1,
layer_index_filter = 12,
aggregation = "mean",
...) {
# Download/select pre-trained BERT model. This will go to an appropriate cache
# directory by default.
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = model
)
# Select all character variables and make then UTF-8 coded, since BERT wants it that way
x_characters <- select_character_v_utf8(x)
# Create lists
BERT_feats <- list()
output_vectors <- list()
# Loop over character variables to tokenize sentences; create BERT-embeddings and Add them to list
for (i in 1:length(x_characters)) {
# Extract BERT feature; help(extract_features) help(make_examples_simple)
BERT_feats[[i]] <- RBERT::extract_features(
examples = x_characters[[i]],
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = layer_indexes_RBERT,
batch_size = batch_size,
...
)
# Extract/Sort output vectors for all sentences...
# Convenience functions for doing this extraction will be added to the RBERT package in the near future.
#output_vectors[[i]] <- BERT_feats[[i]]$output %>%
#  dplyr::filter(token_index == token_index_filter, layer_index == layer_index_filter)
output_vectors[[i]] <- textEmbeddingAggregation(BERT_feats[[i]]$output, aggregation = aggregation)
}
# Gives the names in the list the same name as the orginal character variables
names(output_vectors) <- names(x_characters)
output_vectors
}
wordembeddings2 <- textImportText(gg)
wordembeddings2
