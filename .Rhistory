ctrl <- control_grid(save_pred = TRUE)
#cl <- makeCluster(10)
#registerDoParallel(cl)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
grid = df3_glmn_grid,
control = ctrl
)
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
#  step_BoxCox(all_predictors()) %>%
step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .99) #%>%
# recipes::step_pca(one_of(!!stations), num_comp = tune())
# recipes::prep(training = bivariate_data_train)
df3_recipe
# Cross-validation
df3_cv_splits <- rsample::vfold_cv(df3, v = 10, repeats = 1, strata = NULL) # , ... ,  breaks = 4
df3_cv_splits
# Model
df3_model <-
parsnip::linear_reg(penalty = tune(), mixture = tune()) %>% # tune() uses the grid
parsnip::set_engine("glmnet")
# Tuning; parameters; grid for ridge regression. https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#26
df3_glmn_grid <- base::expand.grid(
penalty = 10 ^ seq(-3, -1, length = 20),
mixture = (0:5) / 5
)
ctrl <- control_grid(save_pred = TRUE)
#cl <- makeCluster(10)
#registerDoParallel(cl)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
grid = df3_glmn_grid,
control = ctrl
)
# Select the best penelty and mixture based on rmsea
# show_best(df3_glmn_tune, metric = "rmse", maximize = FALSE)
best_glmn <-
select_best(df3_glmn_tune, metric = "rmse", maximize = FALSE)
# Get predictions and observed (https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#32)
#df3_predictions <- tune::collect_predictions(df3_easy_eval)
df3_predictions <- tune::collect_predictions(df3_glmn_tune) %>%
filter(penalty == best_glmn$penalty, mixture == best_glmn$mixture)
df3_predictions
# Evaluating the predictions using correlation
correlation <- cor.test(df3_predictions$y, df3_predictions$.pred)
output <- list(correlation, df3_predictions)
names(output) <- c("correlation", "predictions")
output
solmini <- solmini %>%
mutate(phq_tot_diagnose10 = cut(phq_tot, breaks=c(-Inf, 10, Inf), labels=c("0","1")))
solmini$phq_tot_diagnose10
cor.test(as.numeric(solmini$minidep_diagnose), as.numeric(solmini$phq_tot_diagnose10))
chisq.test(solmini$minidep_diagnose, solmini$phq_tot_diagnose10)
output
df3_predictions$.pred
testingdafsd <- df3_predictions$.pred
testingdafsd
testingdafsd <- testingdafsd %>%
mutate(AI_dep_diagnose = cut(testingdafsd, breaks=c(-Inf, 0.5, Inf), labels=c("0","1")))
pred_y <- as_tibble(df3_predictions$.pred, df3_predictions$y)
pred_y <- tibble(df3_predictions$.pred, df3_predictions$y)
pred_y
testingdafsd <- pred_y %>%
mutate(AI_dep_diagnose = cut(`df3_predictions$.pred`, breaks=c(-Inf, 0.5, Inf), labels=c("0","1")))
pred_y
pred_y1 <- pred_y %>%
mutate(AI_dep_diagnose = cut(`df3_predictions$.pred`, breaks=c(-Inf, 0.5, Inf), labels=c("0","1")))
pred_y1
cor.test(as.numeric(solmini$minidep_diagnose), as.numeric(pred_y1$AI_dep_diagnose))
chisq.test(solmini$minidep_diagnose, pred_y1$AI_dep_diagnose)
solmini$minidep_diagnose
pred_y1$AI_dep_diagnose
cor.test(as.numeric(solmini$minidep_diagnose), as.numeric(pred_y1$AI_dep_diagnose))
cross_validated_preds2
cross_validated_preds2$original_row_number
cor.test(solmini_n2$phq_tot, cross_validated_preds2$`dep_all->phq_tot`)
y <- solmini$minidep_diagnose
df2 <- cbind(x, y)
nrow(df2)
df3 <- df2[complete.cases(df2),]
nrow(df3)
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
#  step_BoxCox(all_predictors()) %>%
step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .99) #%>%
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
#  step_BoxCox(all_predictors()) %>%
step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .95) #%>%
# recipes::step_pca(one_of(!!stations), num_comp = tune())
# recipes::prep(training = bivariate_data_train)
df3_recipe
# Cross-validation
df3_cv_splits <- rsample::vfold_cv(df3, v = 10, repeats = 1, strata = NULL) # , ... ,  breaks = 4
df3_cv_splits
# Model
df3_model <-
parsnip::linear_reg(penalty = tune(), mixture = tune()) %>% # tune() uses the grid
parsnip::set_engine("glmnet")
# Tuning; parameters; grid for ridge regression. https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#26
df3_glmn_grid <- base::expand.grid(
penalty = 10 ^ seq(-3, -1, length = 20),
mixture = (0:5) / 5
)
ctrl <- control_grid(save_pred = TRUE)
#cl <- makeCluster(10)
#registerDoParallel(cl)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
grid = df3_glmn_grid,
control = ctrl
)
best_glmn <-
select_best(df3_glmn_tune, metric = "rmse", maximize = FALSE)
# Get predictions and observed (https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#32)
#df3_predictions <- tune::collect_predictions(df3_easy_eval)
df3_predictions <- tune::collect_predictions(df3_glmn_tune) %>%
filter(penalty == best_glmn$penalty, mixture == best_glmn$mixture)
df3_predictions
# Evaluating the predictions using correlation
correlation <- cor.test(df3_predictions$y, df3_predictions$.pred)
output <- list(correlation, df3_predictions)
names(output) <- c("correlation", "predictions")
output
df3_predictions$.pred
df3_predictions$y
df3_predictions$.pred
df3_predictions$.pred
mean(df3_predictions$.pred)
range(df3_predictions$.pred)
# Model
df3_model <-
#parsnip::linear_reg(penalty = tune(), mixture = tune()) %>% # tune() uses the grid
#parsnip::set_engine("glmnet")
rand_forest(mode = "classification", trees = 2000)
# Tuning; parameters; grid for ridge regression. https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#26
df3_glmn_grid <- base::expand.grid(
penalty = 10 ^ seq(-3, -1, length = 20),
mixture = (0:5) / 5
)
ctrl <- control_grid(save_pred = TRUE)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
grid = df3_glmn_grid,
control = ctrl
)
# Model
df3_model <-
#parsnip::linear_reg(penalty = tune(), mixture = tune()) %>% # tune() uses the grid
#parsnip::set_engine("glmnet")
rand_forest(mode = "classification", trees = 2000) %>%
parsnip::set_engine("ranger")
# Tuning; parameters; grid for ridge regression. https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#26
df3_glmn_grid <- base::expand.grid(
penalty = 10 ^ seq(-3, -1, length = 20),
mixture = (0:5) / 5
)
ctrl <- control_grid(save_pred = TRUE)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
grid = df3_glmn_grid,
control = ctrl
)
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
#  step_BoxCox(all_predictors()) %>%
step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .95) #%>%
# recipes::step_pca(one_of(!!stations), num_comp = tune())
# recipes::prep(training = bivariate_data_train)
df3_recipe
# Cross-validation
df3_cv_splits <- rsample::vfold_cv(df3, v = 10, repeats = 1, strata = NULL) # , ... ,  breaks = 4
df3_cv_splits
# Model
df3_model <-
parsnip::linear_reg(penalty = tune(), mixture = tune()) %>% # tune() uses the grid
parsnip::set_engine("glmnet")
# Tuning; parameters; grid for ridge regression. https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#26
df3_glmn_grid <- base::expand.grid(
penalty = 10 ^ seq(-3, -1, length = 20),
mixture = (0:5) / 5
)
ctrl <- control_grid(save_pred = TRUE)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
grid = df3_glmn_grid,
control = ctrl
)
best_glmn <-
select_best(df3_glmn_tune, metric = "rmse", maximize = FALSE)
# Get predictions and observed (https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#32)
#df3_predictions <- tune::collect_predictions(df3_easy_eval)
df3_predictions <- tune::collect_predictions(df3_glmn_tune) %>%
filter(penalty == best_glmn$penalty, mixture == best_glmn$mixture)
df3_predictions
# Select best predictions based on rmse
best_glmn <-
select_best(df3_glmn_tune, metric = "rmse", maximize = FALSE)
# Get predictions and observed (https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#32)
#df3_predictions <- tune::collect_predictions(df3_easy_eval)
df3_predictions <- tune::collect_predictions(df3_glmn_tune) %>%
filter(penalty == best_glmn$penalty, mixture == best_glmn$mixture)
df3_predictions
# Evaluating the predictions using correlation
correlation <- cor.test(df3_predictions$y, df3_predictions$.pred)
output <- list(correlation, df3_predictions)
names(output) <- c("correlation", "predictions")
output
df3
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
#  step_BoxCox(all_predictors()) %>%
recipes::step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .95) #%>%
# recipes::step_pca(one_of(!!stations), num_comp = tune())
# recipes::prep(training = bivariate_data_train)
df3_recipe
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
#  step_BoxCox(all_predictors()) %>%
recipes::step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .95) #%>%
# recipes::step_pca(one_of(!!stations), num_comp = tune())
# recipes::prep(training = bivariate_data_train)
df3_recipe
# Cross-validation
df3_cv_splits <- rsample::vfold_cv(df3, v = 10, repeats = 1, strata = NULL) # , ... ,  breaks = 4
df3_cv_splits
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("ranger") %>%
#  fit(y ~ ., data = df3)
# Tuning; parameters; grid for ridge regression. https://rstudio-conf-2020.github.io/applied-ml/Part_5.html#26
#df3_glmn_grid <- base::expand.grid(
#  penalty = 10 ^ seq(-3, -1, length = 20),
#  mixture = (0:5) / 5
#)
# help("workflow")
#df3_workflow <-
#  workflows::workflow() %>%
#  workflows::add_model(df3_model) %>%
#  workflows::add_recipe(df3_recipe) #%>%
#  add_formula(y ~ .)
# Applying the resampling;  help(fit_resamples)
#df3_easy_eval <- tune::fit_resamples(df3_workflow, resamples = df3_cv_splits,  control = control_resamples(save_pred = TRUE))
#df3_easy_eval
ctrl <- control_grid(save_pred = TRUE)
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("ranger") #%>%
ctrl <- control_grid(save_pred = TRUE)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
#  grid = df3_glmn_grid,
control = ctrl
)
# tune_grid()
df3_glmn_tune <- fit_samples(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
#  grid = df3_glmn_grid,
control = ctrl
)
# tune_grid()
df3_glmn_tune <- tune_grid(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
#  grid = df3_glmn_grid,
control = ctrl
)
# tune_grid()
df3_glmn_tune <- fit_resamples(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
#  grid = df3_glmn_grid,
control = ctrl
)
`.notes`
.notes
wanrings()
warnings()
warnings(`.notes`)
warnings(.notes)
warnings().notes
warnings()`.notes`
# tune_grid()
df3_glmn_tune <- fit_resamples(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
#  grid = df3_glmn_grid,
#  control = ctrl
)
y
df3
# help("workflow")
df3_workflow <-
workflows::workflow() %>%
workflows::add_model(df3_model) %>%
workflows::add_recipe(df3_recipe) #%>%
# Applying the resampling;  help(fit_resamples)
df3_easy_eval <- tune::fit_resamples(df3_workflow, resamples = df3_cv_splits,  control = control_resamples(save_pred = TRUE))
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("ranger") %>%
fit(y ~ ., data = df3)
################
#### Random Forrest: Training Cross-validated data
################
df3$yf <- as.factor(df3$y)
################
#### Random Forrest: Training Cross-validated data
################
df3$y <- as.factor(df3$y)
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
recipes::step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .95) #%>%
df3_recipe
# Cross-validation
df3_cv_splits <- rsample::vfold_cv(df3, v = 10, repeats = 1, strata = NULL) # , ... ,  breaks = 4
df3_cv_splits
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("ranger") %>%
fit(y ~ ., data = df3)
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("randomForest") %>%
fit(y ~ ., data = df3)
# help("workflow")
df3_workflow <-
workflows::workflow() %>%
workflows::add_model(df3_model) %>%
workflows::add_recipe(df3_recipe) #%>%
# help("workflow")
df3_workflow <-
workflows::workflow() %>%
workflows::add_model(df3_model) %>%
workflows::add_recipe(df3_recipe) %>%
add_formula(y ~ .)
# tune_grid()
df3_glmn_tune <- fit_resamples(
df3_recipe,
model = df3_model,
resamples = df3_cv_splits,
#  grid = df3_glmn_grid,
#  control = ctrl
)
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("randomForest") %>%
fit(y ~ ., data = df3)
# help("workflow")
df3_workflow <-
workflows::workflow() %>%
workflows::add_model(df3_model) %>%
workflows::add_recipe(df3_recipe) %>%
add_formula(y ~ .)
# Applying the resampling;  help(fit_resamples)
df3_easy_eval <- tune::fit_resamples(df3_workflow, resamples = df3_cv_splits,  control = control_resamples(save_pred = TRUE))
df3_easy_eval
# Select best predictions based on rmse
best_glmn <-
select_best(df3_easy_eval, metric = "rmse", maximize = FALSE)
# Applying the resampling;  help(fit_resamples)
df3_easy_eval <- tune::fit_resamples(df3_workflow, resamples = df3_cv_splits,  control = control_resamples(save_pred = TRUE))
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("randomForest") %>%
fit(y ~ ., data = df3)
# Model
df3_model <- rand_forest(trees = 100, mode = "classification") %>%
set_engine("randomForest") %>%
fit(y ~ ., data = df3)
# Applying the resampling;  help(fit_resamples)
df3_easy_eval <- tune::fit_resamples(df3_workflow, resamples = df3_cv_splits,  control = control_resamples(save_pred = TRUE))
df3_easy_eval
df3_predictions <- tune::collect_predictions(df3_easy_eval)
df3_predictions
chisq.test(df3_predictions$.pred, df3_predictions$y)
chisq.test(df3_predictions$.pred_class, df3_predictions$y)
################
#### Random Forrest: Training Cross-validated data
################
df3
################
#### Random Forrest: Training Cross-validated data
################
df3 <- df3 %>%
select(-yf)
#recipes: Preprocessing with pca help(recipe) help(step_naomit)
df3_recipe <-
recipes::recipe(y ~ .,
data = df3) %>%
recipes::step_naomit(all_predictors(), skip = TRUE) %>%
recipes::step_center(all_predictors()) %>%
recipes::step_scale(all_predictors()) %>%
recipes::step_pca(all_predictors(), threshold = .95) #%>%
df3_recipe
# Cross-validation
df3_cv_splits <- rsample::vfold_cv(df3, v = 10, repeats = 1, strata = NULL) # , ... ,  breaks = 4
df3_cv_splits
# Model
df3_model <- rand_forest(trees = 1000, mode = "classification") %>%
set_engine("randomForest") %>%
fit(y ~ ., data = df3)
# help("workflow")
df3_workflow <-
workflows::workflow() %>%
workflows::add_model(df3_model) %>%
workflows::add_recipe(df3_recipe) %>%
add_formula(y ~ .)
# Applying the resampling;  help(fit_resamples)
df3_easy_eval <- tune::fit_resamples(df3_workflow, resamples = df3_cv_splits,  control = control_resamples(save_pred = TRUE))
df3_predictions <- tune::collect_predictions(df3_easy_eval)
chisq.test(df3_predictions$.pred_class, df3_predictions$y)
df3_workflow
# Model
df3_model <- rand_forest(trees = 1000, mode = "classification") %>%
set_engine("ranger") %>%
fit(y ~ ., data = df3)
df3
################
#### Random Forrest: Training Cross-validated data
################
df3 <- df3 %>%
select(-yf) %>%
make.names()
df3
################
#### Random Forrest: Training Cross-validated data
################
df3 <- df3 %>%
make.names()
df3$y <- as.factor(df3$y)
df3$y
df3$y
df3
df3
x =  solmini_sd300_tk_mean$movement[1:40,]
x
y = solmini$phq_tot[1:40]
x =  solmini_sd300_tk_mean$movement[1:40,]
y = solmini$phq_tot[1:40]
df1 <- dplyr::select(x, dplyr::starts_with("V"))
set.seed(42)
x1 <- dplyr::select(x, dplyr::starts_with("V"))
df2 <- cbind(x1, y)
nrow(df2)
df3 <- df2#[complete.cases(df2),]
.rs.restartR()
# test data
x =  solmini_sd300_tk_mean$movement[1:40,]
y = solmini$phq_tot[1:40]
