point_size = 0.5
arrow_transparency = 0.1
points_without_words_size = 0.2
points_without_words_alpha = 0.2
legend_title = "SDP"
legend_x_axes_label = "x"
legend_y_axes_label = "y"
legend_x_position = 0.02
legend_y_position = 0.02
legend_h_size = 0.2
legend_w_size = 0.2
legend_title_size = 7
legend_number_size = 2
group_embeddings1 = T
group_embeddings2 = T
projection_embedding = T
aggregated_point_size = 0.8
aggregated_shape = 8
aggregated_color_G1 = "#EAEAEA"
aggregated_color_G2 = "#EAEAEA"
projection_color = "#f542bf"
seed = 1005
explore_words = NULL #c("happy harmony")
explore_words_color = "#ad42f5"
explore_words_point = "ALL_1"
explore_words_aggregation = "mean"
remove_words = c("happy", "sad")
space = NULL
n_contrast_group_color = "black"
n_contrast_group_remove = TRUE
scaling = TRUE
textProjectionPlot_comment <- paste(
"INFORMATION ABOUT THE PROJECTION",
comment(word_data),
"INFORMATION ABOUT THE PLOT",
"word_data =", substitute(word_data),
"k_n_words_to_test =", k_n_words_to_test,
"min_freq_words_test =", min_freq_words_test,
"min_freq_words_plot =", min_freq_words_plot,
"plot_n_words_square =", plot_n_words_square,
"plot_n_words_p =", plot_n_words_p,
"plot_n_word_extreme =", plot_n_word_extreme,
"plot_n_word_frequency =", plot_n_word_frequency,
"plot_n_words_middle =", plot_n_words_middle,
"y_axes =", y_axes,
"p_alpha =", p_alpha,
"p_adjust_method =", p_adjust_method,
"bivariate_color_codes =", paste(bivariate_color_codes, collapse = " "),
"word_size_range =", paste(word_size_range, sep = "-", collapse = " - "),
"position_jitter_hight =", position_jitter_hight,
"position_jitter_width =", position_jitter_width,
"point_size =", point_size,
"arrow_transparency =", point_size,
"points_without_words_size =", points_without_words_size,
"points_without_words_alpha =", points_without_words_alpha,
"legend_x_position =", legend_x_position,
"legend_y_position =", legend_y_position,
"legend_h_size =", legend_h_size,
"legend_w_size =", legend_w_size,
"legend_title_size =", legend_title_size,
"legend_number_size =", legend_number_size
)
set.seed(seed)
# Sorting out axes
x_axes_1 <- "dot.x"
p_values_x <- "p_values_dot.x"
if (y_axes == TRUE) {
y_axes_1 <- "dot.y"
p_values_y <- "p_values_dot.y"
y_axes_values_hide <- FALSE
} else if (y_axes == FALSE) {
y_axes_1 <- NULL
p_values_y <- NULL
y_axes_values_hide <- TRUE
}
### Selecting words to plot
# Computing adjusted p-values with those words selected by min_freq_words_test
word_data_padjusted <- word_data$word_data[word_data$word_data$n >= min_freq_words_test, ]
# Selected Aggregated points
aggregated_embeddings_data <- word_data$word_data[word_data$word_data$n == 0,]
# Computing adjusted p-values with those words selected by: k = sqrt(100*N)
if (k_n_words_to_test == TRUE) {
words_k <- sqrt(100 * word_data$word_data$N_participant_responses[1])
word_data_padjusted <- word_data_padjusted %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:words_k)
}
# word_data_padjusted$p_values_dot.x
word_data_padjusted$adjusted_p_values.x <- stats::p.adjust(purrr::as_vector(word_data_padjusted[, "p_values_dot.x"]), method = p_adjust_method)
word_data1 <- dplyr::left_join(word_data$word_data, word_data_padjusted[, c("words", "adjusted_p_values.x")], by = "words")
# word_data$adjusted_p_values.x
if (is.null(y_axes_1) == FALSE) {
# Computing adjusted p-values
word_data1_padjusted_y <- word_data1[word_data1$n >= min_freq_words_test, ]
word_data1_padjusted_y$adjusted_p_values.y <- stats::p.adjust(purrr::as_vector(word_data1_padjusted_y[, "p_values_dot.y"]), method = p_adjust_method)
word_data1 <- left_join(word_data1, word_data1_padjusted_y[, c("words", "adjusted_p_values.y")], by = "words")
}
# Select only min_freq_words_plot to plot (i.e., after correction of multiple comparison for sig. test)
word_data1 <- word_data1[word_data1$n >= min_freq_words_plot, ]
# Select only words based on square-position; and then top frequency in each "square" (see legend) plot_n_words_square
if (is.null(y_axes_1) == TRUE) {
word_data1 <- word_data1 %>%
dplyr::mutate(square_categories = dplyr::case_when(
dot.x < 0 & adjusted_p_values.x < p_alpha ~ 1,
dot.x < 0 & adjusted_p_values.x > p_alpha ~ 2,
dot.x > 0 & adjusted_p_values.x < p_alpha ~ 3
))
data_p_sq1 <- word_data1[word_data1$square_categories == 1, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq3 <- word_data1[word_data1$square_categories == 3, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq_all <- rbind(data_p_sq1, data_p_sq3) # data_p_sq2,
}
if (is.null(y_axes_1) == FALSE) {
# Categorize words to apply specific color plot_n_words_square=1
word_data1 <- word_data1 %>%
dplyr::mutate(square_categories = dplyr::case_when(
dot.x < 0 & adjusted_p_values.x < p_alpha & dot.y > 0 & adjusted_p_values.y < p_alpha ~ 1,
adjusted_p_values.x > p_alpha & dot.y > 0 & adjusted_p_values.y < p_alpha ~ 2,
dot.x > 0 & adjusted_p_values.x < p_alpha & dot.y > 0 & adjusted_p_values.y < p_alpha ~ 3,
dot.x < 0 & adjusted_p_values.x < p_alpha & adjusted_p_values.y > p_alpha ~ 4,
adjusted_p_values.x > p_alpha & adjusted_p_values.y > p_alpha ~ 5,
dot.x > 0 & adjusted_p_values.x < p_alpha & adjusted_p_values.y > p_alpha ~ 6,
dot.x < 0 & adjusted_p_values.x < p_alpha & dot.y < 0 & adjusted_p_values.y < p_alpha ~ 7,
adjusted_p_values.x > p_alpha & dot.y < 0 & adjusted_p_values.y < p_alpha ~ 8,
dot.x > 0 & adjusted_p_values.x < p_alpha & dot.y < 0 & adjusted_p_values.y < p_alpha ~ 9
))
data_p_sq1 <- word_data1[word_data1$square_categories == 1, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq2 <- word_data1[word_data1$square_categories == 2, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq3 <- word_data1[word_data1$square_categories == 3, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq4 <- word_data1[word_data1$square_categories == 4, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
#  data_p_sq5 <- word_data1[word_data1$square_categories==5, ] %>%
#    dplyr::arrange(-n) %>%
#    dplyr::slice(0:plot_n_words_square)
data_p_sq6 <- word_data1[word_data1$square_categories == 6, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq7 <- word_data1[word_data1$square_categories == 7, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq8 <- word_data1[word_data1$square_categories == 8, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq9 <- word_data1[word_data1$square_categories == 9, ] %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_square)
data_p_sq_all <- rbind(
data_p_sq1, data_p_sq2, data_p_sq3,
data_p_sq4, data_p_sq6, # data_p_sq5,
data_p_sq7, data_p_sq8, data_p_sq9
)
}
# Select only words below alpha; and then top dot.x
data_p_x_neg <- word_data1 %>%
dplyr::filter(adjusted_p_values.x < p_alpha) %>%
dplyr::arrange(dot.x) %>%
dplyr::slice(0:plot_n_words_p)
data_p_x_pos <- word_data1 %>%
dplyr::filter(adjusted_p_values.x < p_alpha) %>%
dplyr::arrange(-dot.x) %>%
dplyr::slice(0:plot_n_words_p)
# Select plot_n_word_extreme and Select plot_n_word_frequency
word_data1_extrem_max_x <- word_data1 %>%
dplyr::arrange(-dot.x) %>%
dplyr::slice(0:plot_n_word_extreme)
word_data1_extrem_min_x <- word_data1 %>%
dplyr::arrange(dot.x) %>%
dplyr::slice(0:plot_n_word_extreme)
word_data1_frequency_x <- word_data1 %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_word_frequency)
# Select the middle range, order according to frequency and then select the plot_n_words_middle = 5
mean_m_sd_x <- mean(word_data1$dot.x, na.rm = TRUE) - (sd(word_data1$dot.x, na.rm = TRUE) / 10)
mean_p_sd_x <- mean(word_data1$dot.x, na.rm = TRUE) + (sd(word_data1$dot.x, na.rm = TRUE) / 10)
word_data1_middle_x <- word_data1 %>%
dplyr::filter(dplyr::between(word_data1$dot.x, mean_m_sd_x, mean_p_sd_x)) %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_middle)
word_data1_x <- word_data1 %>%
dplyr::left_join(data_p_sq_all %>%
dplyr::transmute(words, check_p_square = 1), by = "words") %>%
dplyr::left_join(data_p_x_neg %>%
dplyr::transmute(words, check_p_x_neg = 1), by = "words") %>%
dplyr::left_join(data_p_x_pos %>%
dplyr::transmute(words, check_p_x_pos = 1), by = "words") %>%
dplyr::left_join(word_data1_extrem_max_x %>%
dplyr::transmute(words, check_extreme_max_x = 1), by = "words") %>%
dplyr::left_join(word_data1_extrem_min_x %>%
dplyr::transmute(words, check_extreme_min_x = 1), by = "words") %>%
dplyr::left_join(word_data1_frequency_x %>%
dplyr::transmute(words, check_extreme_frequency_x = 1), by = "words") %>%
dplyr::left_join(word_data1_middle_x %>%
dplyr::transmute(words, check_middle_x = 1), by = "words") %>%
dplyr::mutate(extremes_all_x = rowSums(cbind(
check_p_square, check_p_x_neg, check_p_x_pos, check_extreme_max_x, check_extreme_min_x,
check_extreme_frequency_x, check_middle_x
), na.rm = T))
###### Sort words for y-axes.
if (is.null(y_axes_1) == FALSE) {
# Computing adjusted p-values
# Select only words below alpha; and then top dot.x
data_p_y_neg <- word_data1 %>%
dplyr::filter(adjusted_p_values.y < p_alpha) %>%
dplyr::arrange(dot.y) %>%
dplyr::slice(0:plot_n_words_p)
data_p_y_pos <- word_data1 %>%
dplyr::filter(adjusted_p_values.y < p_alpha) %>%
dplyr::arrange(-dot.y) %>%
dplyr::slice(0:plot_n_words_p)
# Select plot_n_word_extreme and Select plot_n_word_frequency
word_data1_extrem_max_y <- word_data1 %>%
dplyr::arrange(-dot.y) %>%
dplyr::slice(0:plot_n_word_extreme)
word_data1_extrem_min_y <- word_data1 %>%
dplyr::arrange(dot.y) %>%
dplyr::slice(0:plot_n_word_extreme)
word_data1_frequency_y <- word_data1 %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_word_frequency)
# Select the middle range, order according to frequency and then select the plot_n_words_middle =5
mean_m_sd_y <- mean(word_data1$dot.y, na.rm = TRUE) - (sd(word_data1$dot.y, na.rm = TRUE) / 10)
mean_p_sd_y <- mean(word_data1$dot.y, na.rm = TRUE) + (sd(word_data1$dot.y, na.rm = TRUE) / 10)
word_data1_middle_y <- word_data1 %>%
dplyr::filter(dplyr::between(word_data1$dot.y, mean_m_sd_y, mean_p_sd_y)) %>%
dplyr::arrange(-n) %>%
dplyr::slice(0:plot_n_words_middle) # TODO selecting on frequency again. perhaps point to have exact middle?
word_data_all <- word_data1_x %>%
dplyr::left_join(data_p_y_pos %>%
dplyr::transmute(words, check_p_y_pos = 1), by = "words") %>%
dplyr::left_join(data_p_y_neg %>%
dplyr::transmute(words, check_p_y_neg = 1), by = "words") %>%
dplyr::left_join(word_data1_extrem_max_y %>%
dplyr::transmute(words, check_extreme_max_y = 1), by = "words") %>%
dplyr::left_join(word_data1_extrem_min_y %>%
dplyr::transmute(words, check_extreme_min_y = 1), by = "words") %>%
dplyr::left_join(word_data1_frequency_y %>%
dplyr::transmute(words, check_extreme_frequency_y = 1), by = "words") %>%
dplyr::left_join(word_data1_middle_y %>%
dplyr::transmute(words, check_middle_y = 1), by = "words") %>%
dplyr::mutate(extremes_all_y = rowSums(cbind(
check_p_y_neg, check_p_y_pos, check_extreme_max_y, check_extreme_min_y,
check_extreme_frequency_y, check_middle_y
), na.rm = T)) %>%
dplyr::mutate(extremes_all = rowSums(cbind(extremes_all_x, extremes_all_y), na.rm = T))
# Categorize words to apply specific color
word_data_all <- word_data_all %>%
dplyr::mutate(colour_categories = dplyr::case_when(
dot.x < 0 & adjusted_p_values.x < p_alpha & dot.y > 0 & adjusted_p_values.y < p_alpha ~ bivariate_color_codes[1],
adjusted_p_values.x > p_alpha & dot.y > 0 & adjusted_p_values.y < p_alpha ~ bivariate_color_codes[2],
dot.x > 0 & adjusted_p_values.x < p_alpha & dot.y > 0 & adjusted_p_values.y < p_alpha ~ bivariate_color_codes[3],
dot.x < 0 & adjusted_p_values.x < p_alpha & adjusted_p_values.y > p_alpha ~ bivariate_color_codes[4],
adjusted_p_values.x > p_alpha & adjusted_p_values.y > p_alpha ~ bivariate_color_codes[5],
dot.x > 0 & adjusted_p_values.x < p_alpha & adjusted_p_values.y > p_alpha ~ bivariate_color_codes[6],
dot.x < 0 & adjusted_p_values.x < p_alpha & dot.y < 0 & adjusted_p_values.y < p_alpha ~ bivariate_color_codes[7],
adjusted_p_values.x > p_alpha & dot.y < 0 & adjusted_p_values.y < p_alpha ~ bivariate_color_codes[8],
dot.x > 0 & adjusted_p_values.x < p_alpha & dot.y < 0 & adjusted_p_values.y < p_alpha ~ bivariate_color_codes[9]
))
}
if (is.null(y_axes_1) == TRUE) {
word_data_all <- word_data1_x %>%
dplyr::mutate(colour_categories = dplyr::case_when(
dot.x < 0 & adjusted_p_values.x < p_alpha ~ bivariate_color_codes[4],
# dot.x < 0 & adjusted_p_values.x > p_alpha ~ bivariate_color_codes[5],
# Some adjusted_p_values.x has NA becasue they where not tested as multiple input (this is because min_frequency selects out before)
adjusted_p_values.x > p_alpha | is.na(adjusted_p_values.x) ~ bivariate_color_codes[5],
dot.x > 0 & adjusted_p_values.x < p_alpha ~ bivariate_color_codes[6]
))
}
#######
#### Removing and Changing Color of Words Plotted on the side they were less represented in
######
# Colorise words that are more frequent on the opposite side of the dot product projection  n_contrast_group_color = "black"
if(is.character(n_contrast_group_color) == TRUE) {
# Select words with MORE words in G1 and POSITIVE dot product (i.e., remove words that are more represented in the opposite group of its dot product projection)
word_data_all$colour_categories[(abs(word_data_all$n_g1.x) > abs(word_data_all$n_g2.x) & word_data_all$dot.x>0)] <- n_contrast_group_color
# Select words with MORE words in G2 and POSITIVE dot product (i.e., remove words that are more represented in the opposite group of its dot product projection)
word_data_all$colour_categories[(abs(word_data_all$n_g1.x) < abs(word_data_all$n_g2.x) & word_data_all$dot.x<0)] <- n_contrast_group_color
}
# Remove words that are more frequent on the opposite side of the dot product projection
if(n_contrast_group_remove == TRUE) {
word_data_all1  <- word_data_all %>%
# Select words with MORE words in G1 and NEGATIVE dot product (i.e., do not select words that are more represented in the opposite group of its dot product projection)
filter((abs(n_g1.x) > abs(n_g2.x) &
dot.x < 0))
word_data_all2  <- word_data_all %>%
# Select words with MORE words in G2 and POSITIVE dot product (i.e., do not select words that are more represented in the opposite group of its dot product projection)
filter((abs(n_g1.x) < abs(n_g2.x) &
dot.x > 0))
word_data_all <- bind_rows(word_data_all1, word_data_all2)
}
####### words with n_contrast_group
# This solution is because it is not possible to send "0" as a parameter
if (is.null(y_axes_1) == TRUE) {
only_x_dimension <- 0
y_axes_1 <- "only_x_dimension"
}
# Add or Remove values on y-axes
if (y_axes_values_hide) {
y_axes_values <- ggplot2::element_blank()
} else {
y_axes_values <- ggplot2::element_text()
}
# Word data adjusted for if y_axes exists
if (y_axes == TRUE) {
word_data_all_yadjusted <- word_data_all[word_data_all$extremes_all_x >= 1 | word_data_all$extremes_all_y >= 1, ]
} else if (y_axes == FALSE) {
word_data_all_yadjusted <- word_data_all[word_data_all$extremes_all_x >= 1, ]
}
#######
####    Adding words MANUALY
######
if (!is.null(explore_words) == TRUE) {
# For loop for different batches of added words; i_add_w=1 explore_words = "happy harmony love"
forloops_add_w <- length(explore_words)
added_words_information <- list()
for (i_add_w in 1:forloops_add_w) {
# If using a contextualized language model
if(is.null(space) == TRUE){
# Creating word embeddings for the words.
model_text <- sub(".*model: ", '', textProjectionPlot_comment)
model_name <- sub(" layer.*", '', model_text)
layers_text <- sub(".*layers: ", '', textProjectionPlot_comment)
layers_number <- sub(" . textEmbedLayerAggregation.*", '', layers_text)
layers_number_split <- stringr::str_split(layers_number, " ", n = Inf, simplify = FALSE)
explore_words_embeddings <- textEmbed(explore_words[i_add_w],
model = model_name,
layers = dput(as.numeric(layers_number_split[[1]])))
}
# If using a static/decontextualized language model
if(!is.null(space)==TRUE){
explore_words_embeddings <- textEmbedStatic(data.frame(explore_words[i_add_w]),
space = space,
aggregate = explore_words_aggregation)
}
words <- tibble::as_tibble_col(explore_words_point[i_add_w])
colnames(words) <- "words"
n_words <- tibble::as_tibble_col(1)
colnames(n_words) <- "n"
# Scaling embeddings before aggregation
if (scaling == TRUE){
singlewords_we_x <- dplyr::select(explore_words_embeddings$singlewords_we, dplyr::starts_with("Dim"))
# Applying scaling parameters to all the unique word's embeddings
scale_center_weights <- word_data$background[[1]]$scale_centre.x %>%
dplyr::slice(rep(1:dplyr::n(), each=nrow(singlewords_we_x)))
scale_scale_weights <- word_data$background[[1]]$scale_scale.x %>%
dplyr::slice(rep(1:dplyr::n(), each=nrow(singlewords_we_x)))
singlewords_we_x_scaled <- tibble::as_tibble((singlewords_we_x - scale_center_weights)/scale_scale_weights)
singlewords_we_x_scaled_w_n <- bind_cols(explore_words_embeddings$singlewords_we[1:2], singlewords_we_x_scaled)
# Aggregate the words
Aggregated_embedding_added_words <-  tibble::as_tibble_row(textEmbeddingAggregation(singlewords_we_x_scaled, aggregation = explore_words_aggregation))
#Aggregated_embedding_added_words <- as_tibble(t(Aggregated_embedding_added_words))
Mean1 <- dplyr::bind_cols(words, n_words, Aggregated_embedding_added_words)
manual_words_mean1 <- bind_rows(singlewords_we_x_scaled_w_n, Mean1)
} else {
# Aggregate the words
Aggregated_embedding_added_words <-  tibble::as_tibble_row(textEmbeddingAggregation(dplyr::select(explore_words_embeddings$singlewords_we, dplyr::starts_with("Dim")), aggregation = explore_words_aggregation))
#Aggregated_embedding_added_words <- as_tibble(t(Aggregated_embedding_added_words))
Mean1 <- dplyr::bind_cols(words, n_words, Aggregated_embedding_added_words)
manual_words_mean1 <- bind_rows(explore_words_embeddings$singlewords_we, Mean1)
}
###### Project embedding on the x axes
projected_embedding.x <- as.vector(word_data$background[[1]]$Aggregated_word_embedding_group2.x - word_data$background[[1]]$Aggregated_word_embedding_group1.x)
# Position words in relation to Aggregated word embedding
# Position the embedding; i.e., taking the word embedding subtracted with aggregated word embedding
embedding_to_anchour_with.x <- tibble::as_tibble((word_data$background[[1]]$Aggregated_word_embedding_group2.x + word_data$background[[1]]$Aggregated_word_embedding_group1.x)/2)
manual_words_mean1_1.x <- dplyr::select(manual_words_mean1, dplyr::starts_with("Dim"))
embedding_to_anchour_with.x_df <- embedding_to_anchour_with.x %>%
dplyr::slice(rep(1:dplyr::n(), each=nrow(manual_words_mean1_1.x)))
words_positioned_embeddings <- tibble::as_tibble(manual_words_mean1_1.x - embedding_to_anchour_with.x_df)
projected_embedding.x_df <- tibble::as_tibble(projected_embedding.x) %>%
slice(rep(1:dplyr::n(), each = nrow(manual_words_mean1)))
# Project the embeddings using dot product.
#word_data$word_data[word_data$word_data$words == "love",]
dot_products_observed.x <- rowSums(words_positioned_embeddings * projected_embedding.x_df)
### Compare observed dot-product with null
p_values_dot_prod.x <- purrr::map(as.list(purrr::as_vector(dot_products_observed.x)), p_value_comparing_with_Null,
word_data$background[[1]]$dot_null_distribution, alternative = "two_sided")
p_values_dot_prod.x <- unlist(p_values_dot_prod.x)
if(y_axes == TRUE){
###### Project embedding on the Y axes
projected_embedding.y <- word_data$background[[2]]$Aggregated_word_embedding_group2.y - word_data$background[[2]]$Aggregated_word_embedding_group1.y
# Position words in relation to Aggregated word embedding
# Position the embedding; i.e., taking the word embedding subtracted with aggregated word embedding
embedding_to_anchour_with.y <- (word_data$background[[2]]$Aggregated_word_embedding_group2.y + word_data$background[[2]]$Aggregated_word_embedding_group1.y)/2
manual_words_mean1_1.y <- dplyr::select(manual_words_mean1, dplyr::starts_with("Dim"))
words_positioned_embeddings <- manual_words_mean1_1.y - t(replicate(nrow(manual_words_mean1_1.y), as.vector(embedding_to_anchour_with.y)))
# Project the embeddings using dot product.
dot_products_observed.y <- rowSums(words_positioned_embeddings * t(replicate(nrow(manual_words_mean1), projected_embedding.y)))
### Compare observed dot-product with null
p_values_dot_prod.y <- purrr::map(as.list(purrr::as_vector(dot_products_observed.y)), p_value_comparing_with_Null,
word_data$background[[2]]$dot_null_distribution, alternative = "two_sided")
p_values_dot_prod.y <- unlist(p_values_dot_prod.y)
}
# Sort out dataframe
explore_words_results <- manual_words_mean1[1:2]
explore_words_results$dot.x <- dot_products_observed.x
explore_words_results$p_values_dot.x <- p_values_dot_prod.x
explore_words_results$adjusted_p_values.x <- p_values_dot_prod.x
if(y_axes == TRUE){
explore_words_results$dot.y <- dot_products_observed.y
explore_words_results$p_values_dot.y <- p_values_dot_prod.y
explore_words_results$adjusted_p_values.y <- p_values_dot_prod.y
}
explore_words_results$colour_categories <- explore_words_color[i_add_w] #"#e07f6a"   # "#e07f6a", "#EAEAEA", "#40DD52
# TODO; should not have to print extreme?
explore_words_results$extremes_all_x <- rep(NA, nrow(explore_words_results))#c(1, 1, 1)
explore_words_results$n <- rep(mean(word_data_all$n), nrow(explore_words_results)) #c(300, 300, 300)
explore_words_results$n.percent <- rep(0.5, nrow(explore_words_results)) # c(0.5, 0.5, 0.5)
#explore_words_results$n_g1.x <- c(-1, -1, -1)
explore_words_results$n_g2.x <- rep(5, nrow(explore_words_results)) #c(5, 5, 5)
explore_words_results$N_participant_responses <- rep(max(word_data_all$N_participant_responses), nrow(explore_words_results)) #c(40, 40, 40)
added_words_information[[i_add_w]] <- explore_words_results
}
added_words_information_unlist <- dplyr::bind_rows(added_words_information)
word_data_all_yadjusted <- dplyr::bind_rows(word_data_all_yadjusted, added_words_information_unlist)
}
#######
####    Removing words MANUALY
######
word_data_all
words <- c("happy", "sad", "angry", "fun")
x1 <- c(1, 2, 3, 4)
words <- c("happy", "sad", "angry", "fun")
x1 <- c(1, 2, 3, 4)
x2<- c(1, 2, 3, 4)
test1 <- tibble(words, x1, x2)
test1
test2 <- test1 %>% filter(words == "happy")
test2
test2 <- test1 %>% filter(words != "happy")
test2
test2 <- test1 %>% filter(words != remove_words)
test2
remove_words
test2 <- test1 %>% filter(words != remove_words)
test2
remove_words = NULL
test2 <- test1 %>% filter(words != remove_words)
test2
!is.null(remove_words)
library(text)
devtools::document()
remove_words = c("happy", "sad")
if (!is.null(remove_words)){
test2 <- test1 %>% filter(words != remove_words)
}
test2
test2 <- test1 %>% dplyr::filter(words != remove_words)
test2
remove_words
if (!is.null(remove_words)){
test2 <- test1 %>% dplyr::filter(words != remove_words)
}
test2
remove_words = c("happy")
if (!is.null(remove_words)){
test2 <- test1 %>% dplyr::filter(words != remove_words)
}
test2
#
test2
library(text)
remove_words
is.null(remove_words)
#######
####    Removing words MANUALY
######
remove_words <- c("happy", "sad")
if (!is.null(remove_words)){
word_data_all_yadjusted <- word_data_all_yadjusted %>% dplyr::filter(words != remove_words)
}
remove_words
is.null(remove_words)
remove_words
word_data_all_yadjusted <- word_data_all_yadjusted %>% dplyr::filter(words != remove_words)
word_data_all_yadjusted
if (!is.null(remove_words)){
word_data_all_yadjusted <- word_data_all_yadjusted %>% dplyr::filter(words !%in% remove_words)
}
words <- c("happy", "sad", "angry", "fun")
x1 <- c(1, 2, 3, 4)
x2<- c(1, 2, 3, 4)
test1 <- tibble(words, x1, x2)
remove_words = c("happy")
test2 <- test1 %>% filter(words %in% remove_words)
test2
remove_words
test2 <- test1 %>% filter(words %in% !remove_words)
test2 <- test1 %>% filter(!words %in% remove_words)
test2
remove_words = c("happy", "fun")
if (!is.null(remove_words)){
test2 <- test1 %>% filter(!words %in% remove_words)
}
test2
library(text)
library(text)
1239 + 622
335000 - 3578 - 87372 - 78693
library(tidyverse)
library(tidymodels)
library(text)
