normalizeV <- function(x) {
x / sqrt(sum(x^2, na.rm = TRUE))
}
#  devtools::document()
#' Takes all words as input and arrange them in column with an accomponing column with frequency
#'
#' @param x words
#' @return Column with all words and an accomponing column with their frequency.
#' @importFrom tibble as_tibble
#' @noRd
unique_freq_words <- function(words){
words_group <- data.frame(unlist(strsplit(tolower(words), " ")))
words_groupb <- tibble::as_tibble(as.character(words_group[,1]))
words_groupb_freq <- tibble::as_tibble(table(words_groupb))
colnames(words_groupb_freq) <- c("words", "n")
words_groupb_freq
}
#  devtools::document()
#' Create cummalative distribution; Permutates randomly from within group and compute cosine to the group's aggregated embedding
#'
#' @param words_groupX_single_wordembedding_b Each word's word embedding
#' @param Aggregated_word_embedding_groupX The groups aggregated word embedding
#' @param Npermutations Number of permutations (i.e., number of cosines to return)
#' @param n_per_split Number of permutations that should be done in each forloop (too many crashes computer/too few takes a long time)
#' @return Distribution of cosines to the group's aggregated embedding
#' @importFrom tibble as_tibble
#' @noRd
GroupX_cummalative_distribution_cosine <- function(words_groupX_single_wordembedding_b, Aggregated_word_embedding_groupX, Npermutations, n_per_split) {
forloops <- ceiling(Npermutations/n_per_split)
cosine_null_distribution <- list()
for(i in 1:forloops){
# Select random word embeddings accoridng to setting n_per_split = 1000
indice <- sample(nrow(words_groupX_single_wordembedding_b), n_per_split, replace=TRUE)
random_group2_embedding <- words_groupX_single_wordembedding_b[indice, ]
# Compute the cosine between randomly drawn word embeddings and compute the mean
group2_rcosines <- cosines(random_group2_embedding, t(replicate(nrow(random_group2_embedding), Aggregated_word_embedding_groupX)))
cosine_null_distribution[i] <- as_tibble(group2_rcosines)
cosine_null_distribution
}
cosine_null_distribution <- tibble::as_tibble(unlist(cosine_null_distribution))
cosine_null_distribution <- cosine_null_distribution[complete.cases(cosine_null_distribution),]
}
set.seed(2020)
# PCA on single_wordembeddings
if (is.numeric(pca)) {
# Select word embeddings to be included in plot
uniques_words_all <- unique_freq_words(words)
uniques_words_all_wordembedding <- sapply(uniques_words_all$words, applysemrep, single_wordembeddings)
uniques_words_all_wordembedding <- tibble::as_tibble(t(uniques_words_all_wordembedding))
rec_pca <- recipes::recipe( ~ ., data = uniques_words_all_wordembedding)
pca_trans <- rec_pca %>%
recipes::step_center(recipes::all_numeric()) %>%
recipes::step_scale(recipes::all_numeric()) %>%
recipes::step_naomit(V1 , skip = TRUE)
if(pca < 1) {
pca_trans <- recipes::step_pca(pca_trans, recipes::all_numeric(), threshold = pca)
} else if(pca >= 1) {
pca_trans <- recipes::step_pca(pca_trans, recipes::all_numeric(), num_comp = pca)
}
pca_estimates <- recipes::prep(pca_trans, training = uniques_words_all_wordembedding)
pca_data <- recipes::bake(pca_estimates, uniques_words_all_wordembedding)
pca_data <- pca_data %>% stats::setNames(paste0('V_', names(.)))
single_wordembeddings <- dplyr::bind_cols(uniques_words_all, pca_data)
single_wordembeddings
}
pca = NULL
set.seed(2020)
# PCA on single_wordembeddings
if (is.numeric(pca)) {
# Select word embeddings to be included in plot
uniques_words_all <- unique_freq_words(words)
uniques_words_all_wordembedding <- sapply(uniques_words_all$words, applysemrep, single_wordembeddings)
uniques_words_all_wordembedding <- tibble::as_tibble(t(uniques_words_all_wordembedding))
rec_pca <- recipes::recipe( ~ ., data = uniques_words_all_wordembedding)
pca_trans <- rec_pca %>%
recipes::step_center(recipes::all_numeric()) %>%
recipes::step_scale(recipes::all_numeric()) %>%
recipes::step_naomit(V1 , skip = TRUE)
if(pca < 1) {
pca_trans <- recipes::step_pca(pca_trans, recipes::all_numeric(), threshold = pca)
} else if(pca >= 1) {
pca_trans <- recipes::step_pca(pca_trans, recipes::all_numeric(), num_comp = pca)
}
pca_estimates <- recipes::prep(pca_trans, training = uniques_words_all_wordembedding)
pca_data <- recipes::bake(pca_estimates, uniques_words_all_wordembedding)
pca_data <- pca_data %>% stats::setNames(paste0('V_', names(.)))
single_wordembeddings <- dplyr::bind_cols(uniques_words_all, pca_data)
single_wordembeddings
}
# Make datafram (and combine x and y)
if (is.null(y) == TRUE) {
x <- data.frame(x)
} else {
# Combine the dimensions for for-loop
x <- data.frame(x, y)
}
# Creating a list for the x and y dimensions
word_data_list <- list()
# Get the word embeddings and scale/category for the plot dimension (i.e., x or y from above)
x1 <- data.frame(words, x[i_dim])
x1
x2 <- tibble::as_tibble(cbind(x1, wordembeddings)) # Do I really need these aggregated embeddings
# Splitting datasets up to low versus high according to median split
group1 <- x2[ x2[2] < stats::median(x2[[2]]), ]
group2 <- x2[ x2[2] > stats::median(x2[[2]]), ]
# Use function addEqualNrNArows from 3_1_testSimilarity
# Function adds rows of NA until group2 and group1 have the same amount of rows.
if (nrow(group1) < nrow(group2)) {
group1 <- addEqualNrNArows(group1, group2)
} else if (nrow(group1) > nrow(group2)) {
group2 <- addEqualNrNArows(group2, group1)
} else {
group1 <- group1
group2 <- group2
}
##########
####        Get word embeddings
##########
# Group 1: getting unique words and their frequency
words_group1b_freq <- unique_freq_words(group1$words)
words_group1b_freq <- words_group1b_freq[words_group1b_freq$n >= min_freq_words, ]
words_group1b_freq$n_g1_g2 <- words_group1b_freq$n * -1
# Get word embeddings for each word (applysemrep function is created in 1_1_textEmbedd).
words_group1_single_wordembedding <- sapply(words_group1b_freq$words, applysemrep, single_wordembeddings)
words_group1_single_wordembedding_b <- tibble::as_tibble(t(words_group1_single_wordembedding))
# Group 2
words_group2b_freq <- unique_freq_words(group2$words)
words_group2b_freq <- words_group2b_freq[words_group2b_freq$n >= min_freq_words, ]
words_group2b_freq$n_g1_g2 <- words_group2b_freq$n * 1
#  devtools::document()
#' Select all character variables and make then UTF-8 coded, since BERT wants it that way
#'
#' @param tibble including both text and numeric variabes
#' @return all character variables in UTF-8 format.
#' @importFrom dplyr select_if
#' @importFrom tibble as_tibble
#' @importFrom purrr map
#' @importFrom stringi stri_encode
#' @noRd
select_character_v_utf8 <- function(x){
# Select all character variables
x_characters <- dplyr::select_if(x, is.character)
# This makes sure that all variables are UTF-8 coded, since BERT wants it that way
x_characters <- tibble::as_tibble(purrr::map(x_characters, stringi::stri_encode, "", "UTF-8"))
}
#  devtools::document()
#' Function to take min, max, mean or the CLS (which comes from BERT models; not Static spaces) from list of vectors
#'
#' @param x word embeddings to be aggregated
#' @param aggregation method to carry out the aggreation
#' @return aggreagated word ambeddings.
#' @importFrom stats complete.cases
#' @noRd
#'
textEmbeddingAggregation <- function(x, aggregation = "min"){
if(aggregation == "min"){
min_vector <- unlist(map(x, min, na.rm = TRUE))
} else if (aggregation == "max") {
max_vector <- unlist(map(x, max, na.rm = TRUE))
} else if (aggregation == "mean") {
mean_vector <- colMeans(x, na.rm = TRUE)
} else if (aggregation == "CLS"){
CLS <- x %>%
dplyr::filter(token_index == 1, layer_index == 1)
} else if (aggregation == "normalize1") {
#    norma_vector <- unlist(map(x, norma))
x2 <- x[complete.cases(x), ]
x3 <- colSums(x2) # BELOW NEED FIXING; REMOVED FROM CRAN
# x4 <- ppls::normalize.vector(x3)
}
}
# Below are two helper functions not exported for users of the package
# embeddings <- wordembeddings4_10[1]
# words <- c("happy", "harmony", "joy", "sad", "cry", "ad", "afdg", "adgh", "asdfg", "age")
# single_wordembeddings_df <- cbind(words, embeddings$harmonywords)
# single_wordembeddings_df <- as_tibble(single_wordembeddings_df)
# devtools::document()
#' applysemrep
#' Function to apply the semantic representation to ONE word from a matrix of semreps; and
#' return vector with NA if word is not found.
#' That is, look up word embeddings for each word in output_vectors_sw
#' @param x A word
#' @param single_wordembeddings Used to get number of dimensions in embedding/space
#' @return semantic representation from a matrix.
#' @noRd
# x="happy"
# single_wordembeddings_df
applysemrep <- function(x, single_wordembeddings1) {
# If semrep is found get it; if not return NA vector of dimensions
if (sum(single_wordembeddings1$words == x[TRUE]) %in% 1) {
x <- tolower(x)
# Get the semantic representation for a word=x
word1rep <- single_wordembeddings1[single_wordembeddings1$words == x, ]
# Only get the semantic represenation as a vector without the actual word in the first column
wordrep <- purrr::as_vector(word1rep %>% dplyr::select(dplyr::starts_with("V")))
# If the word does not have a semrep return vector with NA the same number of dimensions as columns with V
} else if (x %in% NA) {
# The length() refers to how many column starts with V (i.e., how many dimensions)
wordrep <- data.frame(matrix(ncol = length(single_wordembeddings1 %>% dplyr::select(dplyr::starts_with("V"))), nrow = 1))
class(wordrep)
wordrep <- as.numeric(wordrep)
} else {
wordrep <- data.frame(matrix(ncol = length(single_wordembeddings1 %>% dplyr::select(dplyr::starts_with("V"))), nrow = 1))
wordrep <- as.numeric(wordrep)
}
}
#  devtools::document()
#' semanticrepresentation
#' Function to apply an aggregated semantic representaion for ALL words in a CELL; and if there are no words return a vector with NAs
#' The function is using above applysemrep function.
#' @param x words
#' @param single_wordembeddings Used to get number of dimensions in embedding/space
#' @return semantic representations for all words in cells.
#' @noRd
semanticrepresentation <- function(x, single_wordembeddings2, aggregate = "min", ...) {
x <- tolower(x)
# Separates the words in a cell into a character vector with separate words.
x <- data.frame(unlist(stringr::str_extract_all(x, "[[:alpha:]]+")))
colnames(x) <- c("wordsAll1")
x <- as_tibble(x)
x <- as.character(x$wordsAll1)
# If empty return a "semantic representation" with NA
if (length(x) == 0) {
x2 <- data.frame(matrix(ncol = length(single_wordembeddings2 %>% dplyr::select(dplyr::starts_with("V"))), nrow = 1))
x2 <- as.numeric(x2)
} else {
# Create a matrix with all the semantic representations using the function above
x1 <- sapply(x, applysemrep, ...)
x1 <- tibble::as_tibble(t(x1))  ################## This makes it work for textStaticSpace, but might do something bad with other function?
# If more than one semrep; Sum all the semantic represenations; if not return it as is so that NA etc is returned/kept
x2 <- textEmbeddingAggregation(x1, aggregation = aggregate) #aggregate
# If all values are 0 they should be NA instead; otherwise return the semantic representation.
if (all(x2 == 0|x2 == Inf|x2 == -Inf | is.nan(x2)) == TRUE){
x2 <- data.frame(matrix(ncol = length(single_wordembeddings2 %>% dplyr::select(dplyr::starts_with("V"))), nrow = 1))
#OLD:   x2 <- data.frame(matrix(ncol = length(space)-1, nrow = 1))
x2 <- as.numeric(x2)
} else {
x2 <- x2
}
}
}
# devtools::document()
#' getUniqueWordsAndFreq
#' Function unites several text variables and rows to one, where all text is tranformed to lowercase and tokenized.
#' Also give word frequencies.
#' @param x A word
#' @return dataframe with unique words and their frequency.
#' @noRd
getUniqueWordsAndFreq <- function(x_characters){
# Unite all text variables into one
x_characters2 <- tidyr::unite(x_characters, "x_characters2", 1:ncol(x_characters), sep = " ")
# unite all rows in the column into one cell
x_characters3 <- paste(x_characters2[1], collapse = " ")
# Remove remove all punctuation characters
x_characters4 <- stringr::str_replace_all(x_characters3, "[[:punct:]]", " ")
# Remove  \n
x_characters5 <- gsub("[\r\n]", " ", x_characters4)
x_characters6 <- gsub("[\n]", " ", x_characters5)
# Tokenize into single words
x_characters7 <- tokenizers::tokenize_words(x_characters6, simplify = T)
# Create dataframe with single words and frequency
x_characters8 <- data.frame(sort(table(unlist(strsplit(tolower(x_characters7), " ")))))
singlewords <- tibble(x_characters8$Var1, x_characters8$Freq)
colnames(singlewords) <- c("words", "n")
singlewords$words <- as.character(singlewords$words)
singlewords
}
##########
####        Get word embeddings
##########
# Group 1: getting unique words and their frequency
words_group1b_freq <- unique_freq_words(group1$words)
words_group1b_freq <- words_group1b_freq[words_group1b_freq$n >= min_freq_words, ]
words_group1b_freq$n_g1_g2 <- words_group1b_freq$n * -1
# Get word embeddings for each word (applysemrep function is created in 1_1_textEmbedd).
words_group1_single_wordembedding <- sapply(words_group1b_freq$words, applysemrep, single_wordembeddings)
words_group1_single_wordembedding_b <- tibble::as_tibble(t(words_group1_single_wordembedding))
# Group 2
words_group2b_freq <- unique_freq_words(group2$words)
words_group2b_freq <- words_group2b_freq[words_group2b_freq$n >= min_freq_words, ]
words_group2b_freq$n_g1_g2 <- words_group2b_freq$n * 1
words_group2_single_wordembedding <- sapply(words_group2b_freq$words, applysemrep, single_wordembeddings)
words_group2_single_wordembedding_b <- tibble::as_tibble(t(words_group2_single_wordembedding))
# All: Group 1&2 min_freq_words=1
words_group1_2_freq <- unique_freq_words(x2$words)
words_group1_2_freq_b <- words_group1_2_freq[words_group1_2_freq$n >= min_freq_words, ]
words_group1_2_freq_b <- dplyr::rename(words_group1_2_freq_b, n_all = n)
words_group1_2_single_wordembedding <- sapply(words_group1_2_freq_b$words, applysemrep, single_wordembeddings)
words_group1_2_single_wordembedding_b <- tibble::as_tibble(t(words_group1_2_single_wordembedding))
words_group1_2_single_wordembedding_c <- cbind(words_group1_2_freq_b, words_group1_2_single_wordembedding_b)
# Get lower and upper quartile
z <- x1[,2]
qs <- broom::tidy(summary(z))
group1_agg <- x2[ x2[2] < qs$q1, ]
group2_agg <- x2[ x2[2] > qs$q3, ]
words_group1_agg_freq <- unique_freq_words(group1_agg$words)
words_group1_agg_single_wordembedding <- sapply(words_group1_agg_freq$words, applysemrep, single_wordembeddings)
words_group1_agg_single_wordembedding_b <- tibble::as_tibble(t(words_group1_agg_single_wordembedding))
words_group1_agg_single_wordembedding_c <- cbind(words_group1_agg_freq, words_group1_agg_single_wordembedding_b)
words_group2_agg_freq <- unique_freq_words(group2_agg$words)
words_group2_agg_single_wordembedding <- sapply(words_group2_agg_freq$words, applysemrep, single_wordembeddings)
words_group2_agg_single_wordembedding_b <- tibble::as_tibble(t(words_group2_agg_single_wordembedding))
words_group2_agg_single_wordembedding_c <- cbind(words_group2_agg_freq, words_group2_agg_single_wordembedding_b)
words_group1_agg_single_wordembedding_c <- tibble::as_tibble(words_group1_agg_single_wordembedding_c)
words_group2_agg_single_wordembedding_c <- tibble::as_tibble(words_group2_agg_single_wordembedding_c)
# Weight words for aggregated word embedding: Repeat rows according to n word_weight_power
words_group1_agg_single_wordembedding_d <-  words_group1_agg_single_wordembedding_c %>%
dplyr::mutate(., n1 = n^word_weight_power)  %>%
tidyr::uncount(n1)
words_group2_agg_single_wordembedding_d <-  words_group2_agg_single_wordembedding_c %>%
dplyr::mutate(., n1 = n^word_weight_power)  %>%
tidyr::uncount(n1)
Aggregated_word_embedding_group1 <- textEmbeddingAggregation(dplyr::select(words_group1_agg_single_wordembedding_d, dplyr::starts_with("V")), aggregation = aggregation)
Aggregated_word_embedding_group2 <- textEmbeddingAggregation(dplyr::select(words_group2_agg_single_wordembedding_d, dplyr::starts_with("V")), aggregation = aggregation)
############
######         Project embedding
#############
projected_embedding <- Aggregated_word_embedding_group2 - Aggregated_word_embedding_group1
# Position words in relation to Group 2 (High)
all_unique_words_freq <- unique_freq_words(x2$words)
# Get word embeddings for each word (applysemrep function is created in 1_1_textEmbedd).
all_unique_words_we <- sapply(all_unique_words_freq$words, applysemrep, single_wordembeddings)
all_unique_words_we_b <- tibble::as_tibble(t(all_unique_words_we))
# Position the embedding; i.e., taking the word embedding substrated with aggregated word embedding
#version 1: word_new = word_old - ((group(high harmony) + group(low harmony)) / 2)
words_positioned_embeddings <- all_unique_words_we_b - ((t(replicate(nrow(all_unique_words_we_b), Aggregated_word_embedding_group2)) +
t(replicate(nrow(all_unique_words_we_b), Aggregated_word_embedding_group1)))/2)
# project the embeddings using dot products
dot_products_observed <- rowSums(words_positioned_embeddings * t(replicate(nrow(all_unique_words_we_b), projected_embedding)))
all_unique_words_freq$dot <- dot_products_observed
############
######         Comparison distributions for Project embedding
#############
# Get df with ALL embedding to randomly draw from (without log transformed, and quartiles)
words_group1_agg_single_wordembedding_e <- cbind(words_group1b_freq, words_group1_single_wordembedding_b)
words_group1_agg_single_wordembedding_f <-  words_group1_agg_single_wordembedding_e %>%
dplyr::mutate(., n1_e = n)  %>%
tidyr::uncount(n1_e)
words_group2_agg_single_wordembedding_e <- cbind(words_group2b_freq, words_group2_single_wordembedding_b)
words_group2_agg_single_wordembedding_f <-  words_group2_agg_single_wordembedding_e %>%
dplyr::mutate(., n1_e = n)  %>%
tidyr::uncount(n1_e)
words_group1_2_agg_single_wordembedding_e <- rbind(words_group1_agg_single_wordembedding_f, words_group2_agg_single_wordembedding_f)
words_group1_2_agg_single_wordembedding_e1 <- dplyr::select(words_group1_2_agg_single_wordembedding_e, dplyr::starts_with("V"))
# Splitting up the permutations in different loops to avoid memory issues n_per_split=1000
forloops <- ceiling(Npermutations/n_per_split)
dot_null_distribution <- list()
for(i in 1:forloops){
### Create new Projected embedding
#Randomly split word embeddings into two groups: words_group1_2_agg_single_wordembedding_e1
ind <- sample(c(TRUE, FALSE), nrow(words_group1_2_agg_single_wordembedding_e1), replace=TRUE)
Aggregated_word_embedding_group1_random <- words_group1_2_agg_single_wordembedding_e1[ind, ]
Aggregated_word_embedding_group1_random <- textEmbeddingAggregation(Aggregated_word_embedding_group1_random, aggregation = "mean")
Aggregated_word_embedding_group2_random <- words_group1_2_agg_single_wordembedding_e1[!ind, ]
Aggregated_word_embedding_group2_random <- textEmbeddingAggregation(Aggregated_word_embedding_group2_random, aggregation = "mean")
projected_embedding_random <- Aggregated_word_embedding_group2_random - Aggregated_word_embedding_group1_random
# Select random word embeddings accordings to setting
indice <- sample(nrow(words_group1_2_agg_single_wordembedding_e1), n_per_split, replace=TRUE)
random_group2_embedding <- words_group1_2_agg_single_wordembedding_e1[indice, ]
# Position the embedding; i.e., taking the word embedding substrated with aggregated word embedding
#version 1: word_new = word_old - ((group(high harmony) + group(low harmony)) / 2)
words_positioned_embeddings_random <- random_group2_embedding - ((t(replicate(nrow(random_group2_embedding), Aggregated_word_embedding_group2)) +
t(replicate(nrow(random_group2_embedding), Aggregated_word_embedding_group1)))/2)
#      # version 2: word_new = word_old - mean(all_words)  [1x768] x |words|
#      words_positioned_embeddings_random <- random_group2_embedding - t(replicate(nrow(random_group2_embedding), Aggregated_word_embedding_group1_2a))
# project the embeddings using dot products
dot_products_null <- as_tibble(rowSums(words_positioned_embeddings_random * t(replicate(nrow(words_positioned_embeddings_random), projected_embedding_random)))) # _random to test
dot_null_distribution[i] <- dot_products_null
dot_null_distribution
}
dot_null_distribution <- tibble::as_tibble(unlist(dot_null_distribution))
### Compare observed dot-product with null
dot_null_distribution <- dot_null_distribution[complete.cases(dot_null_distribution),]
p_values_dot_prod <- purrr::map(as.list(purrr::as_vector(dot_products_observed)), p_value_comparing_with_Null,
dot_null_distribution,
Npermutations = Npermutations, alternative = "two_sided")
p_values_dot_prod <- unlist(p_values_dot_prod)
# Sort out dataframe
dot_result <- cbind(all_unique_words_freq, dot_products_observed, tibble::as_tibble(unlist(p_values_dot_prod)))
dot_result <- tibble::as_tibble(dot_result)
colnames(dot_result) <- c("words", "n", "dot", "dot2", "p_values_dot")
dot_result <- dplyr::select(dot_result,-c(dot2))
words_group2b_freq<- dplyr::select(words_group2b_freq,-c(n))
words_group1b_freq<- dplyr::select(words_group1b_freq,-c(n))
dot_result1 <- dplyr::full_join(dot_result, words_group1b_freq, by = "words")
#  devtools::document()
#' Takes all words as input and arrange them in column with an accomponing column with frequency
#'
#' @param x words
#' @return Column with all words and an accomponing column with their frequency.
#' @importFrom tibble as_tibble
#' @noRd
unique_freq_words <- function(words){
words_group <- data.frame(unlist(strsplit(tolower(words), " ")))
words_groupb <- tibble::as_tibble(as.character(words_group[,1]))
words_groupb_freq <- tibble::as_tibble(table(words_groupb))
colnames(words_groupb_freq) <- c("words", "n")
words_groupb_freq
}
#  devtools::document()
#' Create cummalative distribution; Permutates randomly from within group and compute cosine to the group's aggregated embedding
#'
#' @param words_groupX_single_wordembedding_b Each word's word embedding
#' @param Aggregated_word_embedding_groupX The groups aggregated word embedding
#' @param Npermutations Number of permutations (i.e., number of cosines to return)
#' @param n_per_split Number of permutations that should be done in each forloop (too many crashes computer/too few takes a long time)
#' @return Distribution of cosines to the group's aggregated embedding
#' @importFrom tibble as_tibble
#' @noRd
GroupX_cummalative_distribution_cosine <- function(words_groupX_single_wordembedding_b, Aggregated_word_embedding_groupX, Npermutations, n_per_split) {
forloops <- ceiling(Npermutations/n_per_split)
cosine_null_distribution <- list()
for(i in 1:forloops){
# Select random word embeddings accoridng to setting n_per_split = 1000
indice <- sample(nrow(words_groupX_single_wordembedding_b), n_per_split, replace=TRUE)
random_group2_embedding <- words_groupX_single_wordembedding_b[indice, ]
# Compute the cosine between randomly drawn word embeddings and compute the mean
group2_rcosines <- cosines(random_group2_embedding, t(replicate(nrow(random_group2_embedding), Aggregated_word_embedding_groupX)))
cosine_null_distribution[i] <- as_tibble(group2_rcosines)
cosine_null_distribution
}
cosine_null_distribution <- tibble::as_tibble(unlist(cosine_null_distribution))
cosine_null_distribution <- cosine_null_distribution[complete.cases(cosine_null_distribution),]
}
# Examine how the ordered data's mean of the cosine compare with the random data's, null comparison distribution help(switch)
#  devtools::document()
#' p_value_comparing_with_Null
#'
#' @param NULLresults a vector with NULL distribution of estimate (cosines)
#' @param Observedresults a value representing the observed cosine
#' @param Npermutations Number of permutation used in the test
#' @param alternative "two_sided", "greater", "less"
#' @return p_value
#' @noRd
p_value_comparing_with_Null <- function(Observedresults, NULLresults,  Npermutations, alternative = c("two_sided", "less", "greater")){
switch(alternative,
"two_sided" = {
p_value <- 2 * (min(sum(NULLresults < Observedresults), sum(NULLresults > Observedresults)) / sum(!is.na(NULLresults)))
},
"less" = {
p_value <- sum(NULLresults < Observedresults) / sum(!is.na(NULLresults))
},
"greater" = {
p_value <- sum(NULLresults > Observedresults) / sum(!is.na(NULLresults))
}
)
if (!is.na(p_value)) {
if (p_value == 0) { p_value <- 1 / (Npermutations + 1) }
}
return(p_value)
}
plot_data_large_hil <- textPlotData(
words = words_swl_hil,
wordembeddings = wordembeddings_swl_hil,
single_wordembeddings = single_wordembeddings_swl_hil,
x = x_swl_hil,
y = y_swl_hil,
aggregation = "mean",
Npermutations = 1000000,
n_per_split = 100000,
split = "quartile",
word_weight_power = 1,
min_freq_words = 1,
pca=NULL)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
library(text)
library(text)
library(text)
devtools::document()
devtools::document()
library(text)
devtools::document()
library(text)
x <-  c("I am here")
textEmbed(x)
# Run python file with hungface interface
reticulate::source_python("/Users/oscarkjell/Desktop/1 Projects/0 Research/0 text r-package/text/R/huggingface_interface.py")
devtools::document()
library(text)
x <-  c("I am here")
textEmbed(x)
devtools::document()
library(text)
x <-  c("I am here")
textEmbed(x)
devtools::document()
library(text)
x <-  c("I am here")
textEmbed(x)
devtools::document()
library(text)
x <-  c("I am here")
textEmbed(x)
devtools::document()
library(text)
textEmbed(x)
devtools::document()
library(text)
textEmbed(x)
devtools::document()
textEmbed(x)
library(text)
textEmbed(x)
textEmbed(x)
devtools::document()
library(text)
