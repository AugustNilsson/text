---
output: github_document #rmarkdown::html_vignette
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# text

<!-- badges: start -->
<!-- badges: end -->

The language that individuals use contains a wealth of psychological information interesting for research. *Text* analyzes and visualizes text, and their relation to other text or numerical variables. The r-package is based on state-of-the-art techniques from statistics and artificial intelligence, including natural language processing, deep learning and machine learning.



*Text* is created through a collaboration between psychology and computer science to address research needs and ensure state-of-the-art techiniques. It provides powerful functions tailored to test research hypotheses in social and behaviour sciences for both relatively small and large datasets.

### Short installation guide
Most users simply need to run below installation code. 
For those experiencing problems, please see the [Extended Installation Guide](https://www.r-text.org/articles/Extended%20Installation%20Guide.html).

[CRAN](https://CRAN.R-project.org) version:

``` r
install.packages("text")
```

[GitHub](https://github.com/) development version:

``` r
# install.packages("devtools")
devtools::install_github("oscarkjell/text")
```

### Point solution for transforming text to embeddings
Recent significant advances in NLP research have resulted in improved representations of human language (i.e., language models). These language models have produced big performance gains in tasks related to understanding human language. Text are making these SOTA models  easily accessible through an interface to [HuggingFace](https://huggingface.co/transformers/) in Python.

```{r short_word_embedding_example, eval = FALSE, warning=FALSE, message=FALSE}
library(text)
# Transform the text data to BERT word embeddings
wordembeddings <- textEmbed(Language_based_assessment_data_8_10, 
                            model = 'bert-base-uncased')
```

*Text* provides many of the contemporary state-of-the-art language models that are based on deep learning to model word order and context. Multilingual language models can also represent several languages; multilingual BERT comprises *104 different languages*. 

*Table 1. Some of the available language models*
``` {r HF_tabble_short, echo=FALSE, results='asis'}
library(magrittr)

Models <- c("'bert-base-uncased'","**'bert-multilingual-uncased'**","'xlnet-base-cased'",
                                 "'distilbert-base-cased'", "'roberta-base'")

References <- c("[Devline](https://arxiv.org/abs/1810.04805)","**'bert-multilingual-uncased'**","'xlnet-base-cased'",
                                 "'distilbert-base-cased'", "'roberta-base'")

Layers <- c("12","**XX**","XX","XX", "XX")

Language <- c("English","[104 top languages at Wikipedia](https://meta.wikimedia.org/wiki/List_of_Wikipedias)","XX","XX", "XX")

Dimensions <- c(768, 768, 0, 0, 0)

Tables_short <- tibble::tibble(Models, References, Layers, Dimensions, Language)

knitr::kable(Tables_short, caption="", bootstrap_options = c("hover"), full_width = T)
```
  
See [HuggingFace's Github](https://github.com/huggingface/transformers) for a more comprehensive list of models. 

### An end-to-end package
Text also provides functions to analyse the word embeddings, with well-tested machine learning algorithms and statistics. An example is functions plotting statistically significant words in the word embedding space.

```{r DPP_plot, message=FALSE, warning=FALSE}
library(text)
# Use data (DP_projections_HILS_SWLS_100) that have been pre-processed with the textProjectionData function; the preprocessed test-data included in the package is called: DP_projections_HILS_SWLS_100
plot_projection <- textProjectionPlot(
  word_data = DP_projections_HILS_SWLS_100,
  x_axes = "dot.x",
  y_axes = "dot.y",
  p_values_x = "p_values_dot.x",
  p_values_y = "p_values_dot.y",
  title_top = " Dot Product Projection (DPP) of Harmony in life words",
  x_axes_label = "Low vs. High HILS score",
  y_axes_label = "Low vs. High SWLS score",
  position_jitter_hight = 0.5,
  position_jitter_width = 0.8
)
plot_projection

```


