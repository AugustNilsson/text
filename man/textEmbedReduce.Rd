% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/1_3_textEmbedReduce.R
\name{textEmbedReduce}
\alias{textEmbedReduce}
\title{Pre-trained dimension reduction (experimental)}
\usage{
textEmbedReduce(
  embeddings,
  n_dim = NULL,
  scalar =
    "https://raw.githubusercontent.com/adithya8/ContextualEmbeddingDR/master/models/fb20/scalar.csv",
  pca =
    "https://raw.githubusercontent.com/adithya8/ContextualEmbeddingDR/master/models/fb20/rpca_roberta_768_D_20.csv"
)
}
\arguments{
\item{embeddings}{(list) Embedding(s) - including, tokens, texts and/or word_types.}

\item{n_dim}{(numeric) Number of dimensions to reduce to.}

\item{scalar}{(matrix) Default NULL, using scalars from reference (see reference below for more info). Or set own matrix.}

\item{pca}{(matrix) Default NULL, using model from reference (see reference below for more info). Or set own matrix.}
}
\value{
Returns embeddings with reduced number of dimensions.
}
\description{
Pre-trained dimension reduction (experimental)
}
\details{
To use this method please see and cite:
Ganesan, A. V., Matero, M., Ravula, A. R., Vu, H., & Schwartz, H. A. (2021, June).
Empirical evaluation of pre-trained transformers for human-level nlp: The role of sample size and dimensionality.
In Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting (Vol. 2021, p. 4515).
NIH Public Access.
See also https://adithya8.github.io/blog/paper/2021/04/15/Empirical-Evaluation.html
}
\examples{
\donttest{
embeddings <- textEmbedReduce(word_embeddings_4$texts)
}
}
\seealso{
see \code{\link{textEmbed}}
}
