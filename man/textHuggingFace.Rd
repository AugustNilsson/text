% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/1_1_textEmbed.R
\name{textHuggingFace}
\alias{textHuggingFace}
\title{textHuggingFace extracts layers of hidden states (word embeddings) for all character variables in the dataframe.}
\usage{
textHuggingFace(
  x,
  contexts = TRUE,
  decontexts = TRUE,
  pretrained_weights = "bert-base-uncased",
  tokenizer_class = BertTokenizer,
  model_class = BertModel,
  layers = "all",
  return_tokens = TRUE
)
}
\arguments{
\item{x}{Tibble/dataframe with at least one character variables.}

\item{contexts}{provides word embeddings based on contexts (default = TRUE).}

\item{decontexts}{provides word embeddings of single words as input (default = TRUE), which are used for plotting.}

\item{pretrained_weights}{Character specifying pre-trained language model through RBERT. Current options
"bert_base_uncased", "bert_base_cased", "bert_large_uncased",
"bert_large_cased", "bert_large_uncased_wwm", "bert_large_cased_wwm",
"bert_base_multilingual_cased", "bert_base_chinese", "scibert_scivocab_uncased",
"scibert_scivocab_cased", "scibert_basevocab_uncased", and "scibert_basevocab_cased".}

\item{tokenizer_class}{tokenizer that match pretrained_weights}

\item{model_class}{model class that matches pretrained_weights and tokenizer_class}

\item{layers}{most efficient to only extract the ones you need; but can also extract 'all',
and then remove in textLayerAggregation function, when aggregating embeddings.}

\item{return_tokens}{provide the tokens used in transformer model.}
}
\value{
A tibble with tokens, layer identifyer and word embeddings. Note that layer 0 is the input embedding to the transformer
}
\description{
textHuggingFace extracts layers of hidden states (word embeddings) for all character variables in the dataframe.
}
\examples{
\dontrun{
x <- sq_data_tutorial8_10[1:2, 1:2]
wordembeddings <- textHuggingFace(x)
}
}
\seealso{
see \code{\link{textLayerAggregation}} and \code{\link{textEmbed}}
}
