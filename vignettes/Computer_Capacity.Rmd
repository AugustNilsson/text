---
title: "Adapting to Limited Computer Capacity"
output: github_document #rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Computer_Capacity}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


### Managing computationally heavy analyses/functions
NLP and ML techniques can require a huge amount of computational resources. Wolf et al., (2020) exemplify this by pointing out that the RoBERTa languge model: 

>"was trained on 160 GB of text using 1024 32GB V100. On Amazon-Web-Services cloud computing (AWS), such a pretraining would cost approximately 100K USD." (p. 2)  

There are a lot of computations behind the word embeddings. In *text*, the most computationally heavy and time consuming elements are the process of retrieving word embeddings using textHuggingFace (which is also used in textEmbed). Retreiving word embeddings for a standard dataset with a few hundred participants may take between 15 minutes to an hour. Hence, it is worth planning analyses. A few time and resource management advice include: 

1. Before you run the analyses on your entire dataset, ensure that everything first runs smoothly on a small part of the data set (e.g., 20 rows of your data).
2. Take time to better be able to plan and structure your analyses (see code which take tume below).
3. Let analsyes run over a coffe break or over night. So for example, it might be worth retrieving all word embeddings and compute plot data over night.  

```{r timing_Computer_Capacity, eval = FALSE, warning=FALSE, message=FALSE}

library(text)
# Save starting time
T1 <- Sys.time()
textEmbed(Language_based_assessment_data_8_10[1,1], 
          layers = 12, 
          decontexts = FALSE)
# Save stoping time
T2 <- Sys.time()

# Compute time taken to run above function
T2-T1
```

### Your system's capacity
Thinking about your computer's memory capacity becomes important if you have a lot of data and use many layers with many dimensions. For example consider that one sentence of 10 words/tokens, wich are each represented by 12 layers à 768 dimensions results in 92 160 values (i.e., 10 x 12 x 768). To avoid running out of memory and get analyses to run faster, consider to:

1. Only retrieve the layers that you plan to use (e.g.,s ```layers = 11:12```) rather than retreiving all layers (i.e., ```layers = 'all'```). 
2. Only retreive tokens if you are planing to use them; otherwise set ```return_tokens = FALSE```. 
3. Do not ask for decontextualised word embeddings if you are not going to us them (e.g., in plotting).


### References
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., & Funtowicz, M. (2019). [Huggingface’s transformers: State-of-the-art natural language processing.](https://arxiv.org/abs/1910.03771)

